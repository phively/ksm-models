---
title: "01a Contact reports"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Examine contact report data and assess suitability for text modeling. Specifically, 92,480 visit reports were pulled into an xml file on 2019-04-25. 

# Libraries

```{r setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(xml2) # .xml file parsing
library(lubridate) # date formatting
```

# Load data

Apparently the xml file contains lots of invalid entries, so reading it directly is a no-go:

```{r, eval = FALSE}
xml <- read_xml('data/2019-04-25 CAT contact reports.xml')
```

$\color{red}{\text{Error in doc_parse_file(con, encoding = encoding, as_html = as_html, options = options) :}}$
$\color{red}{\text{PCDATA invalid Char value 18 [9]}}$

But reading it in as a string works just fine.

```{r}
data <- read_file('data/2019-04-25 CAT contact reports.xml')
```

```{r, include = FALSE, eval = FALSE}
# Running some tests
a <- str_sub(data, start = 1, end = 10000)

b <- a %>%
  str_replace_all(pattern = '( |  |\\r|\\n)', replacement = ' ') %>%
  str_extract_all(pattern = '<ROW>.*?</ROW>') %>%
  unlist()
```

Now that that's done, I can manually write each tag into a data frame.

```{r, include = FALSE, eval = FALSE}
# More tests
c <- data.frame(
    ID_NUMBER = b %>% str_extract_all(pattern = '(?<=<ID_NUMBER>).*(?=</ID_NUMBER>)') %>%
    unlist()
  , ID_NUMBER_2 = b %>% str_extract_all(pattern = '(?<=<ID_NUMBER_2>).*(?=</ID_NUMBER_2>)') %>%
    unlist()
  , CONTACT_DATE = b %>% str_extract_all(pattern = '(?<=<CONTACT_DATE>).*(?=</CONTACT_DATE>)') %>%
    unlist()
  , CONTACT_FY = b %>% str_extract_all(pattern = '(?<=<CONTACT_FY>).*(?=</CONTACT_FY>)') %>%
    unlist()
  , DESCRIPTION = b %>% str_extract_all(pattern = '(?<=<DESCRIPTION>).*(?=</DESCRIPTION>)') %>%
    unlist()
  , SUMMARY = b %>% str_extract_all(pattern = '(?<=<SUMMARY>).*(?=</SUMMARY>)') %>%
    unlist()
  , stringsAsFactors = FALSE
)

# Try a function
xmler <- function(data, colname) {
  pattern <- paste0('(?<=<', colname, '>).*(?=</', colname, '>)')
  return(
    str_extract_all(data, pattern = pattern)
  )
}

# More tests
d <- data.frame(
    ID_NUMBER = xmler(b, 'ID_NUMBER')
  , ID_NUMBER_2 = xmler(b, 'ID_NUMBER_2')
  , CONTACT_DATE = xmler(b, 'CONTACT_DATE')
  , CONTACT_FY = xmler(b, 'CONTACT_FY')
  , DESCRIPTION = xmler(b, 'DESCRIPTION')
  , SUMMARY = xmler(b, 'SUMMARY')
  , stringsAsFactors = FALSE
)

# Advanced function
xmler <- function(data, colnames) {
  pattern <- paste0('(?<=<', colnames, '>).*(?=</', colnames, '>)')
  return(
    str_extract_all(data, pattern = pattern, simplify = TRUE) %>% t()
  )
}

# Another test
headers <- c('ID_NUMBER', 'ID_NUMBER_2', 'CONTACT_DATE', 'CONTACT_FY', 'DESCRIPTION', 'SUMMARY')
e <- xmler(a, headers) %>%
  data.frame(stringsAsFactors = FALSE)
names(e) <- headers

# Clean up
remove(a, b, c, d, e, xmler, headers)
```

```{r}
# Function to pull out specific elements between <TAG></TAG>
xmler <- function(data, tagnames) {
  tag_pattern <- paste0('(?<=<', tagnames, '>).*(?=</', tagnames, '>)')
  return(
    str_extract_all(data, pattern = tag_pattern, simplify = TRUE) %>% t()
  )
}

# Column names from xml export
headers <- c('ID_NUMBER', 'ID_NUMBER_2', 'CONTACT_DATE', 'CONTACT_FY', 'DESCRIPTION', 'SUMMARY')

# Parse xml into a dataframe
visits <- data %>% xmler(headers) %>%
  data.frame(stringsAsFactors = FALSE)
names(visits) <- headers
```

Finally, update the data with suitable datatypes.

```{r}
visits <- visits %>% mutate(
  CONTACT_FY = CONTACT_FY %>% as.numeric()
  , CONTACT_DATE = CONTACT_DATE %>% mdy()
)
```

# Text corpus creation

Goal: combine all text associated with each ID_NUMBER into a format that can be modeled.

  1) Separate ID_NUMBER and ID_NUMBER_2
  2) Look at exploratory statistics for the raw data
      * e.g. word counts, duplicate description/summary, by year, etc.
  3) Create a merged dataset combining all text for each ID_NUMBER
  4) Consider stopword removal
  5) Look at exploratory statistics for the merged data
      * e.g. word counts, total observations, etc.

Based on the total size of the final dataset it's possible that some methods will not be suitable (topic models if too few documents, word2vec if too few tokens, etc.)

Begin by appending ID_NUMBER_2 entries as their own rows.

```{r}
visits <- rbind(
  # All existing rows
  visits %>% select(-ID_NUMBER_2)
  # All rows with an ID_NUMBER_2
  , visits %>% filter(ID_NUMBER_2 != '') %>% select(-ID_NUMBER) %>% rename(ID_NUMBER = ID_NUMBER_2)
)
```

## Raw data exploration

## Merged data

```{r, echo = FALSE, include = FALSE}
# Example of merging data by ID
# # Merged dataset; for each answer, paste Body into the Body of the question with matching Id
# merged <- bind_rows(
#   posts %>% filter(Name == "Question") %>% select(Id, Body), # Questions
#   posts %>% filter(Name == "Answer") %>% select(Id = ParentId, Body) # Answers
# ) %>% group_by(Id) %>% summarise_each(funs(paste(., collapse = " ")))
```

## Stopwords

## Merged data exploration