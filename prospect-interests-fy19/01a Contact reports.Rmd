---
title: "01a Contact reports"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Examine contact report data and assess suitability for text modeling. Specifically, 92,480 visit reports were pulled into an xml file on 2019-04-25. 

# Libraries

```{r setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(xml2) # .xml file parsing
library(lubridate) # date formatting
```

# Load data

Apparently the xml file contains lots of invalid entries, so reading it directly is a no-go:

```{r, eval = FALSE}
xml <- read_xml('data/2019-04-25 CAT contact reports.xml')
```

$\color{red}{\text{Error in doc_parse_file(con, encoding = encoding, as_html = as_html, options = options) :}}$
$\color{red}{\text{PCDATA invalid Char value 18 [9]}}$

But reading it in as a string works just fine.

```{r}
data <- read_file('data/2019-04-25 CAT contact reports.xml')
```

```{r, include = FALSE, eval = FALSE}
# Running some tests
a <- str_sub(data, start = 1, end = 10000)

b <- a %>%
  str_replace_all(pattern = '( |  |\\r|\\n)', replacement = ' ') %>%
  str_extract_all(pattern = '<ROW>.*?</ROW>') %>%
  unlist()
```

Now that that's done, I can manually write each tag into a data frame.

```{r, include = FALSE, eval = FALSE}
# More tests
c <- data.frame(
    ID_NUMBER = b %>% str_extract_all(pattern = '(?<=<ID_NUMBER>).*(?=</ID_NUMBER>)') %>%
    unlist()
  , ID_NUMBER_2 = b %>% str_extract_all(pattern = '(?<=<ID_NUMBER_2>).*(?=</ID_NUMBER_2>)') %>%
    unlist()
  , CONTACT_DATE = b %>% str_extract_all(pattern = '(?<=<CONTACT_DATE>).*(?=</CONTACT_DATE>)') %>%
    unlist()
  , CONTACT_FY = b %>% str_extract_all(pattern = '(?<=<CONTACT_FY>).*(?=</CONTACT_FY>)') %>%
    unlist()
  , DESCRIPTION = b %>% str_extract_all(pattern = '(?<=<DESCRIPTION>).*(?=</DESCRIPTION>)') %>%
    unlist()
  , SUMMARY = b %>% str_extract_all(pattern = '(?<=<SUMMARY>).*(?=</SUMMARY>)') %>%
    unlist()
  , stringsAsFactors = FALSE
)

# Try a function
xmler <- function(data, colname) {
  pattern <- paste0('(?<=<', colname, '>).*(?=</', colname, '>)')
  return(
    data %>% str_extract_all(pattern = pattern) %>% unlist()
  )
}

# More tests
d <- data.frame(
    ID_NUMBER = xmler(b, 'ID_NUMBER')
  , ID_NUMBER_2 = xmler(b, 'ID_NUMBER_2')
  , CONTACT_DATE = xmler(b, 'CONTACT_DATE')
  , CONTACT_FY = xmler(b, 'CONTACT_FY')
  , DESCRIPTION = xmler(b, 'DESCRIPTION')
  , SUMMARY = xmler(b, 'SUMMARY')
  , stringsAsFactors = FALSE
)

# Advanced function
xmler <- function(data, colnames) {
  pattern <- paste0('(?<=<', colnames, '>).*(?=</', colnames, '>)')
  return(
    str_extract_all(data, pattern = pattern, simplify = TRUE) %>% t()
  )
}

# Another test
headers <- c('ID_NUMBER', 'ID_NUMBER_2', 'CONTACT_DATE', 'CONTACT_FY', 'DESCRIPTION', 'SUMMARY')
e <- xmler(a, headers) %>%
  data.frame(stringsAsFactors = FALSE)
names(e) <- headers

# Clean up
remove(a, b, c, d, e, xmler, headers, baddata)
```

```{r}
# Convert raw data into row data
rowdata <- data %>%
  str_replace_all(pattern = '( |  |\\r|\\n)', replacement = ' ') %>%
  str_extract_all(pattern = '<ROW>.*?</ROW>') %>%
  unlist()
```

```{r}
# Function to pull out specific elements between <TAG></TAG>
xmler <- function(data, tagname) {
  tag_pattern <- paste0('(?<=<', tagname, '>).*(?=</', tagname, '>)')
  return(
    str_extract_all(data, pattern = tag_pattern) %>% unlist()
  )
}

# Create visit data frame
visits <- data.frame(
    ID_NUMBER = xmler(rowdata, 'ID_NUMBER')
  , ID_NUMBER_2 = xmler(rowdata, 'ID_NUMBER_2')
  , CONTACT_DATE = xmler(rowdata, 'CONTACT_DATE')
  , CONTACT_FY = xmler(rowdata, 'CONTACT_FY')
  , DESCRIPTION = xmler(rowdata, 'DESCRIPTION')
  , SUMMARY = xmler(rowdata, 'SUMMARY')
  , stringsAsFactors = FALSE
)
```

Finally, update the data with suitable datatypes.

```{r}
visits <- visits %>% mutate(
  CONTACT_FY = CONTACT_FY %>% as.numeric()
  , CONTACT_DATE = CONTACT_DATE %>% mdy()
)
```

# Text corpus creation

Goal: combine all text associated with each ID_NUMBER into a format that can be modeled.

  1) Separate ID_NUMBER and ID_NUMBER_2
  2) Look at exploratory statistics for the raw data
      * e.g. word counts, duplicate description/summary, by year, etc.
  3) Create a merged dataset combining all text for each ID_NUMBER
  4) Consider stopword removal
  5) Look at exploratory statistics for the merged data
      * e.g. word counts, total observations, etc.

Based on the total size of the final dataset it's possible that some methods will not be suitable (topic models if too few documents, word2vec if too few tokens, etc.)

Begin by appending ID_NUMBER_2 entries as their own rows.

```{r}
visits <- rbind(
  # All existing rows
  visits %>% select(-ID_NUMBER_2)
  # All rows with an ID_NUMBER_2
  , visits %>% filter(ID_NUMBER_2 != '') %>% select(-ID_NUMBER) %>% rename(ID_NUMBER = ID_NUMBER_2)
)
```

## Raw data exploration

```{r}
visits %>% select(CONTACT_DATE, CONTACT_FY) %>% summary()
```


### Contact reports by date

```{r}
visits %>%
  filter(CONTACT_FY > 1800) %>%
  ggplot(aes(x = CONTACT_FY)) +
  annotate('rect', xmin = 2007, xmax = 2020, ymin = 0, ymax = Inf, fill = 'lightblue', alpha = .5) +
  annotate('text', x = 2007, y = -200, label = 'Campaign', hjust = 0, color = 'lightblue') +
  geom_histogram(binwidth = 1, alpha = .75) +
  labs(title = 'Contact reports by fiscal year', x = 'fiscal year', y = 'contact reports')
```

```{r}
visits %>%
  filter(CONTACT_FY >= 2007) %>%
  group_by(mo_yr = round_date(CONTACT_DATE, unit = 'month')) %>%
  summarise(n = length(ID_NUMBER)) %>%
  ggplot(aes(x = mo_yr, y = n)) +
  annotate('rect', xmin = ymd('20070101'), xmax = ymd('20201231')
           , ymin = 0, ymax = Inf, fill = 'lightblue', alpha = .5) +
  annotate('text', x = ymd('20070101'), y = -50, label = 'Campaign', hjust = 0, color = 'lightblue') +
  geom_line(alpha = .75) +
  labs(title = 'Contact reports by month detail', x = 'calendar year', y = 'contact reports') +
  scale_x_date(date_breaks = '1 year', date_labels = '%Y', date_minor_breaks = '1 month') +
  theme(panel.grid.major.x = element_line(color = 'darkgrey'))
```

```{r}
# Fiscal month function
fiscal_mo <- function(mo, start_mo = 9) {
  (mo - start_mo) %% 12 + 1
}
```


```{r}
vdat <- visits %>%
  filter(CONTACT_FY >= 2007) %>%
  group_by(mo = fiscal_mo(month(CONTACT_DATE)), CONTACT_FY) %>%
  summarise(n = length(ID_NUMBER))

vdat %>%
  ggplot(aes(x = mo, y = n)) +
  geom_line(aes(color = factor(CONTACT_FY), group = CONTACT_FY), alpha = .75) +
  geom_line(data = vdat %>% group_by(mo) %>% summarise(n = mean(n)), size = 1, alpha = .25, linetype = 'dotted') +
  labs(title = 'Contact reports by month and fiscal year', x = 'month', y = 'contact reports', color = 'fiscal year') +
  scale_x_continuous(breaks = 1:12, minor_breaks = NULL, labels = month.name[c(9:12, 1:8)] %>% str_sub(1, 3))
```

Historically, the count of contact reports has increased over time (almost certainly a combination of more outreach plus better data collection), and most activity has occurred in September through November and February through May

### Word counts

Word count distribution (log?)

```{r}
visits <- visits %>%
  mutate(
    # Count of words in SUMMARY
    words = str_count(SUMMARY, '\\w+')
    , source = 'summary'
    # If summary was invalid, count words in DESCRIPTION instead
    , source = ifelse(words >= 1, source, 'description') %>% factor()
    , words = ifelse(words >= 1, words, str_count(DESCRIPTION, '\\w+'))
  )
```

```{r}
visits %>%
  ggplot(aes(x = words)) +
  geom_histogram(binwidth = .1) +
  geom_vline(xintercept = mean(visits$words), color = 'blue', linetype = 'dotted', alpha = .5) +
  scale_x_log10() +
  facet_grid(source ~ .)
```

It turns out there are actually a number of observations that only have a description, with no valid visit summary filled in.

```{r, rows.print = 1E3}
vdat <- visits %>%
  group_by(CONTACT_FY, source) %>%
  summarise(n = length(ID_NUMBER)) %>%
  spread(source, n) %>%
  wranglR::ReplaceValues(old.val = NA, new.val = 0) %>%
  mutate(summary_pct = {summary / (description + summary)})

vdat %>%
  ggplot(aes(x = 1:nrow(vdat), y = summary_pct)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:nrow(vdat), labels = vdat$CONTACT_FY) +
  labs(title = 'Percent of contact reports with summary text by year'
       , x = 'fiscal year', y = '% of CRs') +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .1), labels = scales::percent) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5))
```

I wonder what happened in FY 2000? There was a drop in entered summaries.

```{r}
# Word counts by year
visits %>%
  ggplot(aes(x = factor(CONTACT_FY), y = words + 1)) +
  geom_boxplot() +
  scale_y_log10(breaks = 10^(0:10), minor_breaks = NULL) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5)) +
  labs(title = 'Contact report word counts over time'
       , y = 'word count', x = 'fiscal year'
       )
```

Apparently 2000 was also the year people were asked to do more than paste the DESCRIPTION into the SUMMARY.

## Merged data

```{r, echo = FALSE, include = FALSE}
# Example of merging data by ID
# # Merged dataset; for each answer, paste Body into the Body of the question with matching Id
# merged <- bind_rows(
#   posts %>% filter(Name == "Question") %>% select(Id, Body), # Questions
#   posts %>% filter(Name == "Answer") %>% select(Id = ParentId, Body) # Answers
# ) %>% group_by(Id) %>% summarise_each(funs(paste(., collapse = " ")))
```

## Stopwords

## Merged data exploration