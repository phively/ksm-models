---
title: "03 AF 10K Ever Gave Classification"
output: html_notebook
---

# Goal

Differentiate between those who have ever given $10K+ (cash) in a single year and those who haven't, using the variables deemed most important during [screening](https://github.com/phively/ksm-models/blob/master/af-10k-fy17/02%20AF%2010K%20Variable%20Screening.Rmd).

Cross-validated error will be used to compare modeling approaches. This is not intended to be a *great* model, just a starting point of comparison for later iterations.

# Setup

Load [required packages](https://github.com/phively/ksm-models/blob/master/af-10k-fy17/PACKAGES.txt) and scripts.

```{r load_packages}
source('scripts/load_packages.R')
source('scripts/parse_data.R')
```

Generate modeling data.

```{r gen_data}
mdat <- parse_data('data/2017-12-21 AF 10K Model.csv')
source('scripts/parse_modeling_data.R')
```

# Additional exploration

## Bad model

Throw everything in a GLM.

```{r badmodel.splines_test, cache = TRUE}
mbad <- mdat %>%
  glm(GAVE_10K ~ RECORD_STATUS_CODE + ns(RECORD_YR, df = 12) + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE
        + HOUSEHOLD_CONTINENT + BUS_IS_EMPLOYED + HAS_HOME_ADDR + HAS_HOME_PHONE + HAS_HOME_EMAIL
        + GIVING_PLEDGE_ANY + GIVING_PLEDGE_FIRST_YR + ns(GIFTS_ALLOCS_SUPPORTED, df = 1)
        + ns(GIVING_MAX_CASH_YR, df = 3) + ns(GIFTS_CASH, df = 1) + GIFTS_CREDIT_CARD
        + GIFTS_STOCK + ns(GIFT_CLUB_KLC_YRS) + GIFT_CLUB_NU_LDR_YRS + ns(GIFT_CLUB_LOYAL_YRS)
        + VELOCITY_BINS + ns(VELOCITY3_LIN, df = 3)
      , data = .
      , family = binomial)
```

How do predictions vary with `RECORD_YR`?

```{r badmodel.preds_test, cache = TRUE}
preds <- predict(mbad, type = 'response', newdata = data.frame(
    RECORD_STATUS_CODE = 'A'
  , RECORD_YR = 1901:2020
  , PROGRAM_GROUP = 'FT'
  , PREF_ADDR_TYPE_CODE = 'HOM'
  , HOUSEHOLD_CONTINENT = 'North America'
  , BUS_IS_EMPLOYED = 'FALSE'
  , HAS_HOME_ADDR = 'FALSE'
  , HAS_HOME_PHONE = 'FALSE'
  , HAS_HOME_EMAIL = 'FALSE'
  , GIVING_PLEDGE_ANY = 'FALSE'
  , GIVING_PLEDGE_FIRST_YR = 'FALSE'
  , GIFTS_ALLOCS_SUPPORTED = mean(mdat$GIFTS_ALLOCS_SUPPORTED)
  , GIFTS_FYS_SUPPORTED = mean(mdat$GIFTS_FYS_SUPPORTED)
  , GIVING_MAX_CASH_YR = mean(mdat$GIVING_MAX_CASH_YR)
  , GIFTS_CASH = mean(mdat$GIFTS_CASH)
  , GIFTS_CREDIT_CARD = '0'
  , GIFTS_STOCK = '0'
  , GIFT_CLUB_KLC_YRS = mean(mdat$GIFT_CLUB_KLC_YRS)
  , GIFT_CLUB_NU_LDR_YRS = '0'
  , GIFT_CLUB_LOYAL_YRS = mean(mdat$GIFT_CLUB_LOYAL_YRS)
  , VELOCITY3 = mean(mdat$VELOCITY3)
  , VELOCITY_BINS = 'A. Non'
  , VELOCITY3_LIN = mean(mdat$VELOCITY3_LIN)
))
```

```{r badmodel.test_plot, cache = TRUE}
mdat %>% select(GAVE_10K, RECORD_YR) %>% mutate(preds = predict(mbad, type = 'response')) %>%
  left_join(data.frame(RECORD_YR = 1901:2020, expected = preds), by = 'RECORD_YR') %>%
  ggplot(aes(x = RECORD_YR, y = preds, color = factor(GAVE_10K))) +
  geom_point(alpha = .1) +
  geom_line(aes(y = expected), color = 'darkblue') +
  geom_rug() +
  scale_y_log10(breaks = 10^(0:-8)) +
  scale_x_continuous(breaks = seq(1900, 2020, by = 10)) +
  scale_color_manual(values = c('lightgray', 'red'))
```

For a two-class problem the naive decision threshold would be $p \geq 0.5$.

```{r conf.matrix}
conf_matrix <- function(model, newdata = NULL, threshold = .5) {
  results <- data.frame(
    pred = predict(model, newdata = newdata, type = 'response') >= threshold
  )
  if (is.null(newdata)) {
    results$truth = model$y
  } else {
    results$truth = newdata$GAVE_10K
  }
  results_tbl <- table(truth = results$truth, prediction = results$pred)
  error <- (results_tbl[1, 2] + results_tbl[2, 1]) / sum(results_tbl)
  precision <- results_tbl[2, 2] / sum(results_tbl[, 2])
  sensitivity <- results_tbl[2, 2] / sum(results_tbl[2, ])
  return(
    list(
      # Confusion matrix counts
        conf_matrix = results_tbl
      # Confusion matrix percents
      , conf_matrix_pct = results_tbl / nrow(results)
      # Statistics
      , error = error
      , precision = precision
      , sensitivity = sensitivity
      , F1_score = (precision * sensitivity) / (precision + sensitivity)
    )
  )
}
```

Threshold of 0.5.

```{r badmodel.confusion_matrix_naive}
conf_matrix(model = mbad, threshold = .5)
```

The base frequency for `GAVE_10K` is `r {sum(mdat$GAVE_10K) / nrow(mdat)} %>% round(4)`.

```{r badmodel.confusion_matrix}
conf_matrix(model = mbad, threshold = {sum(mdat$GAVE_10K) / nrow(mdat)})
```

## Train/test split

Begin by creating a holdout sample for testing (10% of the data).

```{r make.holdout}
xv <- KFoldXVal(mdat, k = 2, prop = .1, seed = 91471)
holdoutdat <- mdat[xv[[1]], ]
traindat <- mdat[xv[[2]], ]
remove(xv)
```

# Modeling ground rules

* 10-fold cross-validation repeated 5 times.
* Estimate prediction error with out-of-sample classification error.
* $\theta_{1}$ threshold (donor) set to the empirical probability in the cross-validation set (debatable).

```{r ground.rules}
# 10-fold cross-validation
folds = 10
# 5 repetitions
reps = 5
```

# Cross-validated logistic regression model

Logistic regression has given good results on similar problems in the past, so I'll use it for the first approach.

## Baseline model

All explanatory variables with moderate natural spline degrees of freedom.

```{r glm.baseline, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_baseline <- data.frame(
    reps = rep(1:reps, each = folds)
  , folds = rep(1:folds, times = reps)
  , error = 0
  , precision = 0
  , sensitivity = 0
  , F1_score = 0
)
confus_glm_baseline <- rep(list(NULL), times = reps * folds)

# Seed for reproducibility
set.seed(6469548)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~ RECORD_STATUS_CODE
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT + BUS_IS_EMPLOYED
        + HAS_HOME_ADDR + HAS_HOME_PHONE + HAS_HOME_EMAIL + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS + VELOCITY_BINS
        + ns(VELOCITY3_LIN, df = 3)
      # Train while withholding some data
      , data = traindat[-xv[[fold]], ]
      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Write results to errors data frame
    errs_glm_baseline <- errs_glm_baseline %>% mutate(
        error = case_when(
            reps == rep & folds == fold ~ tmpconfus$error
          , TRUE ~ error)
      , precision = case_when(
            reps == rep & folds == fold ~ tmpconfus$precision
          , TRUE ~ precision)
      , sensitivity = case_when(
            reps == rep & folds == fold ~ tmpconfus$sensitivity
          , TRUE ~ sensitivity)
      , F1_score = case_when(
            reps == rep & folds == fold ~ tmpconfus$F1_score
          , TRUE ~ F1_score)
    )
    # Write confusion matrix to list
    confus_glm_baseline[[(rep - 1) * folds + fold]] <- tmpconfus$conf_matrix
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

Histograms of results.

```{r error.histogrammer}
histogrammer <- function(errordata, varname, h = .005, fill = 'gray') {
  data.frame(x = errordata[, varname]) %>%
  ggplot(aes(x = x)) +
    geom_histogram(aes(y = ..density..), alpha = .5, binwidth = h, fill = fill) +
    geom_density(alpha = .5) +
    geom_vline(aes(xintercept = mean(x), color = 'mean')) +
    geom_vline(aes(xintercept = median(x), color = 'median'), linetype = 'dashed') +
    labs(x = varname)
}
```

```{r glm.baseline.plots}
grid.arrange(
    histogrammer(errs_glm_baseline, 'error', h = .0005, fill = 'pink')
  , histogrammer(errs_glm_baseline, 'precision', h = .005, fill = 'cyan')
  , histogrammer(errs_glm_baseline, 'sensitivity', h = .005, fill = 'green')
)
```

The numbers to beat are:

```{r glm.baseline.stats}
errs_glm_baseline %>% select(error, precision, sensitivity) %>% colMeans() %>% round(4)
```


## Trimmed model

How does dropping apparently less-useful variables affect cross-validated error?

```{r glm.baseline.fulldata}
mbase <- glm(GAVE_10K ~ RECORD_STATUS_CODE
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT + BUS_IS_EMPLOYED
        + HAS_HOME_ADDR + HAS_HOME_PHONE + HAS_HOME_EMAIL + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS + VELOCITY_BINS
        + ns(VELOCITY3_LIN, df = 3)
      , data = traindat
      , family = binomial)
summary(mbase)
```

Compare errors dropping `RECORD_STATUS_CODE`, `BUS_IS_EMPLOYED`, `HAS_HOME_ADDR`, `HAS_HOME_PHONE`, `HAS_HOME_EMAIL`.

```{r glm.trimmed, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_trimmed <- data.frame(
    reps = rep(1:reps, each = folds)
  , folds = rep(1:folds, times = reps)
  , error = 0
  , precision = 0
  , sensitivity = 0
  , F1_score = 0
)
confus_glm_trimmed <- rep(list(NULL), times = reps * folds)

# Seed for reproducibility
set.seed(8755737)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS + VELOCITY_BINS
        + ns(VELOCITY3_LIN, df = 3)
      # Train while withholding some data
      , data = traindat[-xv[[fold]], ]
      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Write results to errors data frame
    errs_glm_trimmed <- errs_glm_trimmed %>% mutate(
        error = case_when(
            reps == rep & folds == fold ~ tmpconfus$error
          , TRUE ~ error)
      , precision = case_when(
            reps == rep & folds == fold ~ tmpconfus$precision
          , TRUE ~ precision)
      , sensitivity = case_when(
            reps == rep & folds == fold ~ tmpconfus$sensitivity
          , TRUE ~ sensitivity)
      , F1_score = case_when(
            reps == rep & folds == fold ~ tmpconfus$F1_score
          , TRUE ~ F1_score)
    )
    # Write confusion matrix to list
    confus_glm_trimmed[[(rep - 1) * folds + fold]] <- tmpconfus$conf_matrix
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

```{r glm.compare2}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
)
```

This appears to be moving in the right direction.

```{r glm.trimmed1.fulldata}
mtrim <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS + VELOCITY_BINS
        + ns(VELOCITY3_LIN, df = 3)
      , data = traindat
      , family = binomial)
summary(mtrim)
```

## Trimmed 2

Try without `VELOCITY_BINS`.

```{r glm.trimmed2, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_trimmed2 <- data.frame(
    reps = rep(1:reps, each = folds)
  , folds = rep(1:folds, times = reps)
  , error = 0
  , precision = 0
  , sensitivity = 0
  , F1_score = 0
)
confus_glm_trimmed2 <- rep(list(NULL), times = reps * folds)

# Seed for reproducibility
set.seed(2362746)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
        + ns(VELOCITY3_LIN, df = 3)
      # Train while withholding some data
      , data = traindat[-xv[[fold]], ]
      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Write results to errors data frame
    errs_glm_trimmed2 <- errs_glm_trimmed2 %>% mutate(
        error = case_when(
            reps == rep & folds == fold ~ tmpconfus$error
          , TRUE ~ error)
      , precision = case_when(
            reps == rep & folds == fold ~ tmpconfus$precision
          , TRUE ~ precision)
      , sensitivity = case_when(
            reps == rep & folds == fold ~ tmpconfus$sensitivity
          , TRUE ~ sensitivity)
      , F1_score = case_when(
            reps == rep & folds == fold ~ tmpconfus$F1_score
          , TRUE ~ F1_score)
    )
    # Write confusion matrix to list
    confus_glm_trimmed2[[(rep - 1) * folds + fold]] <- tmpconfus$conf_matrix
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

```{r glm.compare3}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2 = errs_glm_trimmed2 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
)
```

How about dropping the spline and keeping the factor?

```{r glm.trimmed2b, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_trimmed2b <- data.frame(
    reps = rep(1:reps, each = folds)
  , folds = rep(1:folds, times = reps)
  , error = 0
  , precision = 0
  , sensitivity = 0
  , F1_score = 0
)
confus_glm_trimmed2b <- rep(list(NULL), times = reps * folds)

# Seed for reproducibility
set.seed(702305)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS + VELOCITY_BINS
      # Train while withholding some data
      , data = traindat[-xv[[fold]], ]
      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Write results to errors data frame
    errs_glm_trimmed2b <- errs_glm_trimmed2b %>% mutate(
        error = case_when(
            reps == rep & folds == fold ~ tmpconfus$error
          , TRUE ~ error)
      , precision = case_when(
            reps == rep & folds == fold ~ tmpconfus$precision
          , TRUE ~ precision)
      , sensitivity = case_when(
            reps == rep & folds == fold ~ tmpconfus$sensitivity
          , TRUE ~ sensitivity)
      , F1_score = case_when(
            reps == rep & folds == fold ~ tmpconfus$F1_score
          , TRUE ~ F1_score)
    )
    # Write confusion matrix to list
    confus_glm_trimmed2b[[(rep - 1) * folds + fold]] <- tmpconfus$conf_matrix
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

```{r glm.compare4}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2 = errs_glm_trimmed2 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2b = errs_glm_trimmed2b %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
)
```

Trades off precision for sensitivity. Not a big fan of how the velocity bin coefficients bounce around; from exploration I am not convinced by discretization.

```{r glm.trimmed2.fulldata}
mtrim2 <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + ns(GIVING_MAX_CASH_YR, df = 10)
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
        + ns(VELOCITY3_LIN, df = 3)
      , data = traindat
      , family = binomial)
summary(mtrim2)
```

## Trimmed 3

Drop `GIVING_MAX_CASH_YR`.

```{r glm.trimmed3, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_trimmed3 <- data.frame(
    reps = rep(1:reps, each = folds)
  , folds = rep(1:folds, times = reps)
  , error = 0
  , precision = 0
  , sensitivity = 0
  , F1_score = 0
)
confus_glm_trimmed3 <- rep(list(NULL), times = reps * folds)

# Seed for reproducibility
set.seed(702305)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
        + ns(VELOCITY3_LIN, df = 3)
      # Train while withholding some data
      , data = traindat[-xv[[fold]], ]
      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Write results to errors data frame
    errs_glm_trimmed3 <- errs_glm_trimmed3 %>% mutate(
        error = case_when(
            reps == rep & folds == fold ~ tmpconfus$error
          , TRUE ~ error)
      , precision = case_when(
            reps == rep & folds == fold ~ tmpconfus$precision
          , TRUE ~ precision)
      , sensitivity = case_when(
            reps == rep & folds == fold ~ tmpconfus$sensitivity
          , TRUE ~ sensitivity)
      , F1_score = case_when(
            reps == rep & folds == fold ~ tmpconfus$F1_score
          , TRUE ~ F1_score)
    )
    # Write confusion matrix to list
    confus_glm_trimmed3[[(rep - 1) * folds + fold]] <- tmpconfus$conf_matrix
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

```{r glm.compare5}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2 = errs_glm_trimmed2 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed3 = errs_glm_trimmed3 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
)
```

```{r glm.trimmed3.fulldata}
mtrim3 <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + GIFTS_CASH + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
        + ns(VELOCITY3_LIN, df = 3)
      , data = traindat
      , family = binomial)
summary(mtrim3)
```

## Trimmed 4

Drop `GIFTS_CASH`

```{r glm.trimmed4, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_trimmed4 <- data.frame(
    reps = rep(1:reps, each = folds)
  , folds = rep(1:folds, times = reps)
  , error = 0
  , precision = 0
  , sensitivity = 0
  , F1_score = 0
)
confus_glm_trimmed4 <- rep(list(NULL), times = reps * folds)

# Seed for reproducibility
set.seed(995439)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~
        + ns(RECORD_YR, df = 10)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
        + ns(VELOCITY3_LIN, df = 3)
      # Train while withholding some data
      , data = traindat[-xv[[fold]], ]
      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Write results to errors data frame
    errs_glm_trimmed4 <- errs_glm_trimmed4 %>% mutate(
        error = case_when(
            reps == rep & folds == fold ~ tmpconfus$error
          , TRUE ~ error)
      , precision = case_when(
            reps == rep & folds == fold ~ tmpconfus$precision
          , TRUE ~ precision)
      , sensitivity = case_when(
            reps == rep & folds == fold ~ tmpconfus$sensitivity
          , TRUE ~ sensitivity)
      , F1_score = case_when(
            reps == rep & folds == fold ~ tmpconfus$F1_score
          , TRUE ~ F1_score)
    )
    # Write confusion matrix to list
    confus_glm_trimmed4[[(rep - 1) * folds + fold]] <- tmpconfus$conf_matrix
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

```{r glm.compare6}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2 = errs_glm_trimmed2 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed3 = errs_glm_trimmed3 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed4 = errs_glm_trimmed4 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
)
```

# Spline models

## Spline degrees of freedom

I'll use natural splines on the numeric predictors but each variable should have some maximum degrees of freedom.

```{r spline.test, cache = TRUE}
mdat %>%
  glm(GAVE_10K ~
        + ns(RECORD_YR, df = 20)
        + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
        + GIVING_PLEDGE_ANY
        + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
        + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
        + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
        + ns(VELOCITY3_LIN, df = 5)
      , data = .
      , family = binomial) %>%
  summary()
```

Proposed grid search:

`RECORD_YR` $\in \left\{5, 10, 15, 20 \right\}$
`VELOCITY3_LIN` $\in \left\{2, 3, 4, 5 \right\}$

This gives 4 * 4 = 16 possibilities.

```{r grid.params}
# Splines grid search parameters
grid_params = list(
    RECORD_YR = c(5, 10, 15, 20)
  , VELOCITY3_LIN = c(2, 3, 4, 5)
)
```

## Spine df grid search

For parallelization: set number of workers to ncores - 1.

```{r parallel.cores}
registerDoParallel(detectCores() - 1)
```

```{r glm.splines, cache = TRUE, warning = FALSE, message = FALSE}
# Store model errors
errs_glm_splines <- data.frame(NULL)

# Seed for reproducibility
set.seed(5544999)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  for (fold in 1:length(xv)) {
    # Status report
    paste(' + Fold', fold, 'beginning at', Sys.time()) %>% print()
    # Go through grid search parameters (parallel)
    prlout <- foreach (yr = grid_params$RECORD_YR, .combine = rbind) %:%
      foreach(vel = grid_params$VELOCITY3_LIN
                , .combine = rbind
                , .packages = c('dplyr', 'splines')
              ) %dopar% {
        # Fit temp model
        tmpmodel <- glm(GAVE_10K ~
                          + ns(RECORD_YR, df = yr)
                          + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
                          + GIVING_PLEDGE_ANY
                          + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
                          + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
                          + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
                          + ns(VELOCITY3_LIN, df = vel)
                          # Train while withholding some data
                          , data = traindat[-xv[[fold]], ]
                          , family = binomial) %>% suppressMessages()
        # Prediction threshold
        theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
        # Confusion matrix based on the withheld data
        tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
        # Return results
        return(
          data.frame(
              reps = rep
            , folds = fold
            , yr_df = yr
            , vel_df = vel
            , error = tmpconfus$error
            , precision = tmpconfus$precision
            , sensitivity = tmpconfus$sensitivity
            , F1_score = tmpconfus$F1_score
          )
        )
      }
    # Write results to errors data frame
    errs_glm_splines <- errs_glm_splines %>% rbind(prlout)
    # Status report
    paste(' -- Fold', fold, 'ending at', Sys.time()) %>% print()
  }
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

Plot results.

```{r spline_gridplotter}
gridplotter <- function(data, var) {
  data %>% mutate(z = data[, var]) %>%
    group_by(vel_df, yr_df) %>% 
    summarise(z = mean(z)) %>%
    ggplot(aes(x = vel_df, y = yr_df)) +
    geom_tile(aes(fill = z)) +
    geom_text(aes(label = z %>% round(4))) +
    labs(title = var)
}
```

```{r spline_grid.err}
gridplotter(errs_glm_splines, 'error')
```
```{r spline_grid.prec}
gridplotter(errs_glm_splines, 'precision')
```
```{r spline_grid.sens}
gridplotter(errs_glm_splines, 'sensitivity')
```
```{r spline_grid.F1}
gridplotter(errs_glm_splines, 'F1_score')
```

Looks like vel_df of 4 and 5 are indistinguishable, so let's set to 4.

```{r splines_yr.sens.plot}
set.seed(123)
errs_glm_splines %>% filter(vel_df == 4) %>%
  ggplot(aes(x = yr_df, y = sensitivity)) +
  geom_jitter(alpha = .5) +
  geom_boxplot(aes(group = yr_df), alpha = .1) +
  geom_smooth(method = 'gam') +
  labs(title = 'sensitivity')
```

```{r splines_yr.f1.plot}
set.seed(123)
errs_glm_splines %>% filter(vel_df == 4) %>%
  ggplot(aes(x = yr_df, y = F1_score)) +
  geom_jitter(alpha = .5) +
  geom_boxplot(aes(group = yr_df), alpha = .25) +
  geom_smooth(method = 'gam') +
  labs(title = 'F1')
```

Looks like fewer df might actually have a slight advantage?

```{r spline.stat.test}
errs_glm_splines %>% filter(vel_df == 4) %>%
  aov(F1_score ~ yr_df, data = .) %>% summary()
```

No apparent difference; let's go with 5 df.

```{r glm.compare7}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2 = errs_glm_trimmed2 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed3 = errs_glm_trimmed3 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed4 = errs_glm_trimmed4 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , splines = errs_glm_splines %>% filter(yr_df == 5 & vel_df == 4) %>%
    select(error, precision, sensitivity) %>% colMeans() %>% round(4)
)
```

Potential concern: is `VELOCITY3_LIN` with moderate to high df too correlated?

```{r velocity3_lin.plot}
traindat %>% select(GAVE_10K, VELOCITY3_LIN) %>%
  ggplot(aes(x = VELOCITY3_LIN, y = GAVE_10K, color = GAVE_10K)) +
  geom_point()
```

On the other hand, we do want to pay special attention to upgrades/downgrades.

# Coefficient shrinkage

## Non-penalized GLM

```{r glm.nonpen.xv}
# Store model errors
errs_glm_nonpen <- data.frame(NULL)

# Seed for reproducibility
set.seed(9860543)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  foreach (fold = 1:length(xv)) %dopar% {
    # Fit temp model
    tmpmodel <- glm(GAVE_10K ~
                        ns(RECORD_YR, df = 5)
                      + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
                      + GIVING_PLEDGE_ANY
                      + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
                      + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
                      + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
                      + ns(VELOCITY3_LIN, df = 4)
                      # Train while withholding some data
                      , data = traindat[-xv[[fold]], ]
                      , family = binomial) %>% suppressMessages()
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Return results
    return(
      data.frame(
          reps = rep
        , folds = fold
        , yr_df = yr
        , vel_df = vel
        , error = tmpconfus$error
        , precision = tmpconfus$precision
        , sensitivity = tmpconfus$sensitivity
        , F1_score = tmpconfus$F1_score
      )
    )
  }
  # Write results to errors data frame
  errs_glm_nonpen <- errs_glm_nonpen %>% rbind(prlout)
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

## Penalized GLM

Need a new function to construct a glmnet confusion matrix.

```{r glmnet.confusion}
conf_matrix_glmnet <- function(model, newdata = NULL, threshold = .5) {
  results <- data.frame(
    predict(model, newdata = newdata, type = 'response'
            , s = 'lambda.1se') >= threshold
  ) %>% select(pred = X1)
  if (is.null(newdata)) {
    results$truth = model$y
  } else {
    results$truth = newdata$GAVE_10K
  }
  results_tbl <- table(truth = results$truth, prediction = results$pred)
  error <- (results_tbl[1, 2] + results_tbl[2, 1]) / sum(results_tbl)
  precision <- results_tbl[2, 2] / sum(results_tbl[, 2])
  sensitivity <- results_tbl[2, 2] / sum(results_tbl[2, ])
  return(
    list(
      # Confusion matrix counts
        conf_matrix = results_tbl
      # Confusion matrix percents
      , conf_matrix_pct = results_tbl / nrow(results)
      # Statistics
      , error = error
      , precision = precision
      , sensitivity = sensitivity
      , F1_score = (precision * sensitivity) / (precision + sensitivity)
    )
  )
}
```

```{r glmnet.xv}
# Store model errors
errs_glm_pen <- data.frame(NULL)

# Seed for reproducibility
set.seed(8774272)

for (rep in 1:reps) {
  # Status report
  paste('+ Iteration', rep, 'beginning at', Sys.time()) %>% print()
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  foreach (fold = 1:length(xv)
           , .packages = c('glmnet', 'glmnetUtils', 'dplyr', 'splines')) %dopar% {
    # alpha = 0 is the ridge regression penalty
    tmpmodel <- cv.glmnet(GAVE_10K ~
                           ns(RECORD_YR, df = 5)
                         + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
                         + GIVING_PLEDGE_ANY
                         + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
                         + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
                         + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
                         + ns(VELOCITY3_LIN, df = 4)
                         # Train while withholding some data
                         , data = traindat[-xv[[fold]], ]
                         , family = 'binomial'
                         , alpha = 0
                         , lambda = 2^(-8:5))
    # Prediction threshold
    theta1 <- sum(traindat$GAVE_10K[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix_glmnet(tmpmodel, newdata = traindat[xv[[fold]], ], threshold = theta1)
    # Return results
    return(
     data.frame(
       reps = rep
       , folds = fold
       , yr_df = yr
       , vel_df = vel
       , error = tmpconfus$error
       , precision = tmpconfus$precision
       , sensitivity = tmpconfus$sensitivity
       , F1_score = tmpconfus$F1_score
     )
    )
    }
  # Write results to errors data frame
  errs_glm_pen <- errs_glm_pen %>% rbind(prlout)
  # Status report
  paste('-- Iteration', rep, 'ending at', Sys.time()) %>% print()
}
```

```{r glm.compare8}
data.frame(
    baseline = errs_glm_baseline %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed = errs_glm_trimmed %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed2 = errs_glm_trimmed2 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed3 = errs_glm_trimmed3 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , trimmed4 = errs_glm_trimmed4 %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , splines = errs_glm_splines %>% filter(yr_df == 5 & vel_df == 4) %>%
    select(error, precision, sensitivity) %>% colMeans() %>% round(4)
  , nonpen = errs_glm_nonpen %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
  , pen = errs_glm_pen %>% select(error, precision, sensitivity)
    %>% colMeans() %>% round(4)
)
```

```{r penmods}
mnpen <- glm(GAVE_10K ~
                        ns(RECORD_YR, df = 5)
                      + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
                      + GIVING_PLEDGE_ANY
                      + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
                      + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
                      + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
                      + ns(VELOCITY3_LIN, df = 4)
                      , data = traindat
                      , family = binomial)
mpen <- cv.glmnet(GAVE_10K ~
                           ns(RECORD_YR, df = 5)
                         + PROGRAM_GROUP + PREF_ADDR_TYPE_CODE + HOUSEHOLD_CONTINENT
                         + GIVING_PLEDGE_ANY
                         + GIVING_PLEDGE_FIRST_YR + GIFTS_ALLOCS_SUPPORTED
                         + GIFTS_CREDIT_CARD + GIFTS_STOCK + GIFT_CLUB_KLC_YRS
                         + GIFT_CLUB_NU_LDR_YRS + GIFT_CLUB_LOYAL_YRS
                         + ns(VELOCITY3_LIN, df = 4)
                         , data = traindat
                         , alpha = 0)
```

```{r pen.coef.compare, warning = FALSE, message = FALSE}
full_join(
    data.frame(var = coef(mnpen) %>% names(), nopen = coef(mnpen))
  , data.frame(var = coef(mpen)[, 1] %>% names(), pen = coef(mpen)[, 1])
)
```

# Holdout data face-off

It's now time for the moment we've all been waiting for! Error statistics will now be computed for each model on the heretofore unseen holdout data.

```{r holdout.faceoff}
# predict(mnpen, newdata = holdoutdat, type = 'response') %>% head()
# predict(mpen, newdata = holdoutdat, type = 'response', s = 'lambda.1se') %>% head()
```