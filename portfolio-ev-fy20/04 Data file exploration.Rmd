---
title: "04 Data file exploration"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Explore the tens of thousands of fields created by dfs. Presumably the resulting feature matrix is very sparse. Which merit a closer look?

# Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(e1071)
library(lubridate)
library(wranglR)
library(readxl)
library(foreach)
library(caret)
library(glmnet)
```

```{r}
# Load data
load('data/output/dfs_output_fy18.Rdata')
```

# Initial exploration

Build a summary matrix describing key attributes of the features.

```{r}
function_class_helper <- function(data, FUN) {
  fun <- match.fun(FUN)
  if (is.numeric(data)) {
    fun(data, na.rm = TRUE)
  } else if (is.factor(data)) {
    fun(summary(data), na.rm = TRUE)
  } else {
    NA
  }
}

function_count_helper <- function(data, lower = -Inf, upper = Inf, excl_upper_bound = TRUE) {
  # If excl_upper_bound then use < rather than <=
  lt <- if(excl_upper_bound) {
    match.fun('<')
  } else {
    match.fun('<=')
  }
  if (is.numeric(data)) {
    sum(data >= lower & lt(data, upper), na.rm = TRUE)
  } else if (is.factor(data)) {
    sum(summary(data) >= lower & lt(summary(data), upper), na.rm = TRUE)
  } else {
    NA
  }
}

summary_df <- function(data) {
  output <- data.frame(
    var = colnames(data)
    , class = sapply(data, class)
  ) %>%
  # General summaries
  mutate(
    n = nrow(data)
    , n_NAs = sapply(data, function(x) sum(is.na(x)))
    , n_nonNAs = n - n_NAs
    , n_fac_levels = sapply(data, function(x) ifelse(is.factor(x), length(levels(x)), NA))
  ) %>%
  # Factor or numeric summaries
  mutate(
    n_zero = sapply(data, function(x) function_count_helper(x, lower = 0, upper = 0, excl_upper_bound = FALSE))
    , n_lt_10 = sapply(data, function(x) function_count_helper(x, upper = 10, excl_upper_bound = TRUE))
    , n_lt_100 = sapply(data, function(x) function_count_helper(x, upper = 100, excl_upper_bound = TRUE))
    , min = sapply(data, function(x) {function_class_helper(x, min)})
    , mean = sapply(data, function(x) {function_class_helper(x, mean)})
    , median = sapply(data, function(x) {function_class_helper(x, median)})
    , max = sapply(data, function(x) {function_class_helper(x, max)})
    , sd = sapply(data, function(x) {function_class_helper(x, sd)})
    , skewness = sapply(data, function(x) {function_class_helper(x, skewness)})
    , kurtosis = sapply(data, function(x) {function_class_helper(x, kurtosis)})
  )
  return(output)
}
```

```{r, warning = FALSE}
# Test my summary_df function
a <- data.frame(
  seq = 1:10
  , seqNAs = c(seq(-40, 120, by = 20), NA)
  , fac = factor(c(1, 1, 1, 2, 2, 2, 3, 3, 3, 1))
  , letters = as.character(letters[1:10])
  , seqsq = (1:10)^2
  , mean0 = seq(-4.5, 4.5, by = 1)
  , has0 = rep(-2:2, each = 2)
)

summary(a)
```

```{r}
summary_df(a)
```

```{r, warning = FALSE}
data_summary <- summary_df(dfs_output_fy18)

save('data_summary', file = 'data/output/dfs_output_fy18_summary.Rdata')
```

```{r}
data_summary %>%
  mutate(
    pct_nas = n_NAs / n
  ) %>%
  ggplot(aes(x = pct_nas)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by % NA data', x = '% NA')
```

```{r}
data_summary %>%
  mutate(
    pct_zero = n_zero / n_nonNAs
  ) %>%
  ggplot(aes(x = pct_zero)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by % zeros', x = '% zero')
```

```{r}
data_summary %>%
  mutate(
    pct_0_or_NA = (n_zero + n_NAs) / n
  ) %>%
  ggplot(aes(x = pct_0_or_NA)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by < 10', x = '% < 10')
```

```{r}
data_summary %>%
  mutate(
    pct_lt_10 = n_lt_10 / n_nonNAs
  ) %>%
  ggplot(aes(x = pct_lt_10)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by < 10', x = '% < 10')
```

A number of features are mostly (90%+) NAs or 0, depending on data type. Looking at overall counts the vast majority of features are either mostly low or mostly high. Not clear what this should mean in terms of including/excluding different variables.

```{r}
data_summary %>%
  ggplot(aes(x = sd)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature standard deviations') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

I assume the very low standard deviation features are the ones where most observations are 0.

```{r}
data_summary %>%
  ggplot(aes(x = mean)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature means') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

```{r}
data_summary %>%
  mutate(
    sd_range = case_when(
      sd == 0 ~ 0
      , round(sd, 1) >= 1 & round(sd, 1) < 2 ~ 1
      , round(sd, 1) >= 2 ~ 2
      , TRUE ~ round(sd, 1)
    ) %>% factor() %>%
      fct_recode(`~0` = '0', `~1` = '1', `2+` = '2')
  ) %>%
  ggplot(aes(x = sd_range)) +
  geom_bar() +
  labs(title = 'Approximate SD distributions')
```

A very large proportion of the valid standard deviations are close to 0.

```{r}
data_summary %>%
  ggplot(aes(x = median)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature medians') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

The means and medians actually look very similar, which I find a bit surprising.

```{r}
data_summary %>%
  ggplot(aes(x = min)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature mins') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

```{r}
data_summary %>%
  ggplot(aes(x = max)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature maxes') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

Mostly 0 for both, surprisingly.

```{r}
data_summary %>%
  ggplot(aes(x = skewness)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature skewness')
```

```{r}
data_summary %>%
  ggplot(aes(x = kurtosis)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature kurtosis') +
  scale_x_sqrt()
```

# Load data and response variable

```{r}
load('data/output/dfs_output_fy18.Rdata')
load('data/output/output_fy18_rv.Rdata')
```

```{r}
rv <- dfs_output_fy18 %>%
  select(HOUSEHOLD_ID) %>%
  left_join(
    output_fy18_rv
    , by = c('HOUSEHOLD_ID' = 'HOUSEHOLD_ID')
  ) %>%
  # Fill in NA with 0
  mutate_if(
    is.numeric
    , function(x) ReplaceValues(x)
  ) %>%
  # Transformed amounts
  mutate(
    gave_any = fy19_cash + fy19_frp > 0
    , gave_cash = fy19_cash > 0
    , gave_frp = fy19_frp > 0
    , fy19_log_cash = log10(fy19_cash + 1)
    , fy19_log_frp = log10(fy19_frp + 1)
  )
```

```{r}
# Check that the order is the same
paste(sum(dfs_output_fy18$HOUSEHOLD_ID != rv$HOUSEHOLD_ID), 'mismatched')
```

# Data cleanup

I'll perform variable pre-screening by computing the AUC for each predictor against an indicator, as recommended by the [University of Melbourne](http://proceedings.mlr.press/v7/miller09/miller09.pdf).

```{r}

```

# caret

Use the caret package to clean up the feature matrix.

```{r}
# Use pagefile
utils::memory.limit(128000)

# Check variances
caret_vars <- nearZeroVar(dfs_output_fy18[, 1:10], saveMetrics = TRUE)
```

https://topepo.github.io/caret/pre-processing.html

# Feature importance

With such a high number of features I'm using the LASSO method as implemented by glmnet to help identify the most interesting ones.

```{r}
# Use pagefile
utils::memory.limit(128000)

# Sample rows
prop = 1/1000 # Proportion of data to sample
set.seed(60952)
samp <- sample_n(dfs_output_fy18, size = nrow(dfs_output_fy18) * prop) %>%
  # Drop list type
  select_if(
    ~ !is.list(.)
  )

dollars <- samp %>% select(HOUSEHOLD_ID) %>%
  left_join(
    output_fy18_rv %>% filter(HOUSEHOLD_ID %in% samp$HOUSEHOLD_ID)
    , by = c('HOUSEHOLD_ID' = 'HOUSEHOLD_ID')
  ) %>%
  # Fill in NA with 0
  mutate_if(
    is.numeric
    , function(x) ReplaceValues(x)
  )
```

```{r}
# Check that the order is the same
paste(sum(samp$HOUSEHOLD_ID != dollars$HOUSEHOLD_ID), 'mismatched')
```

```{r}
# glmnet LASSO
tmp <- data.frame(fy19_frp = dollars$fy19_frp, samp[, 1:100]) %>%
  na.omit()

m_lasso <- glmnet(
  x = model.matrix(fy19_frp ~ ., data = tmp)[, -1]
  , y = log10(tmp$fy19_frp + 1)
  , family = 'gaussian'
  , alpha = 1
)
```
