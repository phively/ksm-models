---
title: "04 Data file exploration"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Explore the tens of thousands of fields created by dfs. Presumably the resulting feature matrix is very sparse. Which merit a closer look?

# Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(e1071)
library(lubridate)
library(wranglR)
library(readxl)
library(foreach)
library(doParallel)
library(caret)
library(glmnet)
```

```{r}
# Load data
load('data/output/dfs_output_fy18.Rdata')
```

# Initial exploration

Build a summary matrix describing key attributes of the features.

```{r}
function_class_helper <- function(data, FUN) {
  fun <- match.fun(FUN)
  if (is.numeric(data)) {
    fun(data, na.rm = TRUE)
  } else if (is.factor(data)) {
    fun(summary(data), na.rm = TRUE)
  } else {
    NA
  }
}

function_count_helper <- function(data, lower = -Inf, upper = Inf, excl_upper_bound = TRUE) {
  # If excl_upper_bound then use < rather than <=
  lt <- if(excl_upper_bound) {
    match.fun('<')
  } else {
    match.fun('<=')
  }
  if (is.numeric(data)) {
    sum(data >= lower & lt(data, upper), na.rm = TRUE)
  } else if (is.factor(data)) {
    sum(summary(data) >= lower & lt(summary(data), upper), na.rm = TRUE)
  } else {
    NA
  }
}

summary_df <- function(data) {
  output <- data.frame(
    var = colnames(data)
    , class = sapply(data, class)
  ) %>%
  # General summaries
  mutate(
    n = nrow(data)
    , n_NAs = sapply(data, function(x) sum(is.na(x)))
    , n_nonNAs = n - n_NAs
    , n_fac_levels = sapply(data, function(x) ifelse(is.factor(x), length(levels(x)), NA))
  ) %>%
  # Factor or numeric summaries
  mutate(
    n_zero = sapply(data, function(x) function_count_helper(x, lower = 0, upper = 0, excl_upper_bound = FALSE))
    , n_lt_10 = sapply(data, function(x) function_count_helper(x, upper = 10, excl_upper_bound = TRUE))
    , n_lt_100 = sapply(data, function(x) function_count_helper(x, upper = 100, excl_upper_bound = TRUE))
    , min = sapply(data, function(x) {function_class_helper(x, min)})
    , mean = sapply(data, function(x) {function_class_helper(x, mean)})
    , median = sapply(data, function(x) {function_class_helper(x, median)})
    , max = sapply(data, function(x) {function_class_helper(x, max)})
    , sd = sapply(data, function(x) {function_class_helper(x, sd)})
    , skewness = sapply(data, function(x) {function_class_helper(x, skewness)})
    , kurtosis = sapply(data, function(x) {function_class_helper(x, kurtosis)})
  )
  return(output)
}
```

```{r, warning = FALSE}
# Test my summary_df function
a <- data.frame(
  seq = 1:10
  , seqNAs = c(seq(-40, 120, by = 20), NA)
  , fac = factor(c(1, 1, 1, 2, 2, 2, 3, 3, 3, 1))
  , letters = as.character(letters[1:10])
  , seqsq = (1:10)^2
  , mean0 = seq(-4.5, 4.5, by = 1)
  , has0 = rep(-2:2, each = 2)
)

summary(a)
```

```{r}
summary_df(a)
```

```{r, warning = FALSE}
data_summary <- summary_df(dfs_output_fy18)

save('data_summary', file = 'data/output/dfs_output_fy18_summary.Rdata')
```

```{r}
data_summary %>%
  mutate(
    pct_nas = n_NAs / n
  ) %>%
  ggplot(aes(x = pct_nas)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by % NA data', x = '% NA')
```

```{r}
data_summary %>%
  mutate(
    pct_zero = n_zero / n_nonNAs
  ) %>%
  ggplot(aes(x = pct_zero)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by % zeros', x = '% zero')
```

```{r}
data_summary %>%
  mutate(
    pct_0_or_NA = (n_zero + n_NAs) / n
  ) %>%
  ggplot(aes(x = pct_0_or_NA)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features 0 or NA', x = '% 0 or NA')
```

```{r}
data_summary %>%
  mutate(
    pct_lt_10 = n_lt_10 / n_nonNAs
  ) %>%
  ggplot(aes(x = pct_lt_10)) +
  geom_histogram(binwidth = .01) +
  scale_x_continuous(breaks = seq(0, 1, by = .1) %>% round(1), label = scales::percent) +
  labs(title = 'Count of features by < 10', x = '% < 10')
```

A number of features are mostly (90%+) NAs or 0, depending on data type. Looking at overall counts the vast majority of features are either mostly low or mostly high. Not clear what this should mean in terms of including/excluding different variables.

```{r}
data_summary %>%
  ggplot(aes(x = sd)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature standard deviations') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

I assume the very low standard deviation features are the ones where most observations are 0.

```{r}
data_summary %>%
  ggplot(aes(x = mean)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature means') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

```{r}
data_summary %>%
  mutate(
    sd_range = case_when(
      sd == 0 ~ 0
      , round(sd, 1) >= 1 & round(sd, 1) < 2 ~ 1
      , round(sd, 1) >= 2 ~ 2
      , TRUE ~ round(sd, 1)
    ) %>% factor() %>%
      fct_recode(`~0` = '0', `~1` = '1', `2+` = '2')
  ) %>%
  ggplot(aes(x = sd_range)) +
  geom_bar() +
  labs(title = 'Approximate SD distributions')
```

A very large proportion of the valid standard deviations are close to 0.

```{r}
data_summary %>%
  ggplot(aes(x = median)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature medians') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

The means and medians actually look very similar, which I find a bit surprising.

```{r}
data_summary %>%
  ggplot(aes(x = min)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature mins') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

```{r}
data_summary %>%
  ggplot(aes(x = max)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature maxes') +
  scale_x_log10(breaks = 10^seq(-20, 20, by = 2))
```

Mostly 0 for both, surprisingly.

```{r}
data_summary %>%
  ggplot(aes(x = skewness)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature skewness')
```

```{r}
data_summary %>%
  ggplot(aes(x = kurtosis)) +
  geom_histogram(bins = 30) +
  labs(title = 'Distribution of feature kurtosis') +
  scale_x_sqrt()
```

# Load data and response variable

```{r}
load('data/output/dfs_output_fy18_noNA_nolists.Rdata')
load('data/output/output_fy18_rv.Rdata')
```

```{r}
rv <- dfs_output_fy18 %>%
  select(HOUSEHOLD_ID) %>%
  left_join(
    output_fy18_rv
    , by = c('HOUSEHOLD_ID' = 'HOUSEHOLD_ID')
  ) %>%
  # Fill in NA with 0
  mutate_if(
    is.numeric
    , function(x) ReplaceValues(x)
  ) %>%
  # Transformed amounts
  mutate(
    gave_any = fy19_cash + fy19_frp > 0
    , gave_cash = fy19_cash > 0
    , gave_frp = fy19_frp > 0
    , fy19_log_cash = log10(fy19_cash + 1)
    , fy19_log_frp = log10(fy19_frp + 1)
  )
```

```{r}
# Check that the order is the same
paste(sum(dfs_output_fy18$HOUSEHOLD_ID != rv$HOUSEHOLD_ID), 'mismatched')
```

# Caret benchmarking

Use the caret package to clean up the feature matrix. Begin by checking for low-variance predictors.

Begin with some benchmarking.

```{r}
nzv_benchmarker <- function(saveMetrics = TRUE, foreach = FALSE, allowParallel = TRUE) {
  time0 <- Sys.time()
  nearZeroVar(dfs_output_fy18[, 1:10], saveMetrics = saveMetrics, foreach = foreach, allowParallel = allowParallel)
  time10 <- Sys.time()
  nearZeroVar(dfs_output_fy18[, 1:20], saveMetrics = saveMetrics, foreach = foreach, allowParallel = allowParallel)
  time20 <- Sys.time()
  nearZeroVar(dfs_output_fy18[, 1:30], saveMetrics = saveMetrics, foreach = foreach, allowParallel = allowParallel)
  time30 <- Sys.time()
  nearZeroVar(dfs_output_fy18[, 1:40], saveMetrics = saveMetrics, foreach = foreach, allowParallel = allowParallel)
  time40 <- Sys.time()
  nearZeroVar(dfs_output_fy18[, 1:50], saveMetrics = saveMetrics, foreach = foreach, allowParallel = allowParallel)
  time50 <- Sys.time()
  return(
    c(
      time10 - time0
      , time20 - time10
      , time30 - time20
      , time40 - time30
      , time50 - time40
    )
  )
}

n_vars <- seq(10, 50, by = 10)
```

```{r}
nzv_timing <- data.frame(
  vars = n_vars
  , time = nzv_benchmarker()
)
```
```{r}
nzv_timing %>%
  ggplot(aes(x = vars, y = time)) +
  geom_point() +
  geom_smooth(method = 'lm', alpha = .5)
```

Linear scaling with the number of variables, which is...expected but not a hopeful sign.

```{r}
# Hours to run
{nzv_timing %>% lm(time ~ vars, data = .) %>% predict(newdata = data.frame(vars = ncol(dfs_output_fy18)))} / 3600
```

```{r}
# Benchmarking in parallel
nzv_timing_parallel <- data.frame(
  vars = n_vars
  , time = nzv_benchmarker(foreach = TRUE)
)
```
```{r}
nzv_timing_parallel %>%
  ggplot(aes(x = vars, y = time)) +
  geom_point() +
  geom_smooth(method = 'lm', alpha = .5)
```

```{r}
# Hours to run
{nzv_timing_parallel %>% lm(time ~ vars, data = .) %>% predict(newdata = data.frame(vars = ncol(dfs_output_fy18)))} / 3600
```

Parallel processing cuts down the time needed by over 50% - not too shabby!

```{r}
# Test scaling with different core counts
c2 <- makePSOCKcluster(2)
c4 <- makePSOCKcluster(5)
c6 <- makePSOCKcluster(6)
c8 <- makePSOCKcluster(8)
c10 <- makePSOCKcluster(10)

nzv_timing_cores <- data.frame()

registerDoParallel(c2)
nzv_timing_cores <- nzv_timing_cores %>% rbind(data.frame(cores = 2, vars = n_vars, time = nzv_benchmarker(foreach = TRUE)))
stopCluster(c2)
registerDoParallel(c4)
nzv_timing_cores <- nzv_timing_cores %>% rbind(data.frame(cores = 4, vars = n_vars, time = nzv_benchmarker(foreach = TRUE)))
stopCluster(c4)
registerDoParallel(c6)
nzv_timing_cores <- nzv_timing_cores %>% rbind(data.frame(cores = 6, vars = n_vars, time = nzv_benchmarker(foreach = TRUE)))
stopCluster(c6)
registerDoParallel(c8)
nzv_timing_cores <- nzv_timing_cores %>% rbind(data.frame(cores = 8, vars = n_vars, time = nzv_benchmarker(foreach = TRUE)))
stopCluster(c8)
registerDoParallel(c10)
nzv_timing_cores <- nzv_timing_cores %>% rbind(data.frame(cores = 10, vars = n_vars, time = nzv_benchmarker(foreach = TRUE)))
stopCluster(c10)
```

```{r}
nzv_timing_cores %>%
  ggplot(aes(x = vars, y = time, color = factor(cores))) +
  geom_point() +
  geom_smooth(method = 'lm', alpha = .2)
```

Interesting...try it with more vars.

```{r}
time0 <- Sys.time()
c2 <- makePSOCKcluster(2)
registerDoParallel(c2)
a <- nearZeroVar(dfs_output_fy18[, 1:300], saveMetrics = TRUE, foreach = TRUE, allowParallel = TRUE)
time2 <- Sys.time()
stopCluster(c2)

c4 <- makePSOCKcluster(5)
registerDoParallel(c4)
a <- nearZeroVar(dfs_output_fy18[, 1:300], saveMetrics = TRUE, foreach = TRUE, allowParallel = TRUE)
time4 <- Sys.time()
stopCluster(c4)

c6 <- makePSOCKcluster(6)
registerDoParallel(c6)
a <- nearZeroVar(dfs_output_fy18[, 1:300], saveMetrics = TRUE, foreach = TRUE, allowParallel = TRUE)
time6 <- Sys.time()
stopCluster(c6)

c8 <- makePSOCKcluster(8)
registerDoParallel(c8)
a <- nearZeroVar(dfs_output_fy18[, 1:300], saveMetrics = TRUE, foreach = TRUE, allowParallel = TRUE)
time8 <- Sys.time()
stopCluster(c8)

c10 <- makePSOCKcluster(10)
registerDoParallel(c10)
a <- nearZeroVar(dfs_output_fy18[, 1:300], saveMetrics = TRUE, foreach = TRUE, allowParallel = TRUE)
time10 <- Sys.time()
stopCluster(c10)

nzv_cores_300 <- data.frame(cores = seq(2, 10, by = 2), time = c(time2 - time0, time4 - time2, time6 - time4, time8 - time6, time10 - time8))
```

```{r}
nzv_cores_300 %>%
  ggplot(aes(x = cores, y = time)) +
  geom_point() +
  geom_smooth(method = 'lm', alpha = .5)
```

4, 6 or 8 cores all appear to be reasonable.

# Near-zero variance run

```{r}
# Use pagefile
utils::memory.limit(128000)

# Set up parallel cluster
c6 <- makePSOCKcluster(6)
registerDoParallel(c6)

# Check variances
caret_vars <- nearZeroVar(dfs_output_fy18, saveMetrics = TRUE, foreach = TRUE, allowParallel = TRUE)

save('caret_vars', file = 'data/output/caret_fy18.Rdata')

stopCluster(c6)
```

https://topepo.github.io/caret/pre-processing.html

# Feature importance

With such a high number of features I'm using the LASSO method as implemented by glmnet to help identify the most interesting ones.

```{r}
# Use pagefile
utils::memory.limit(128000)

# Sample rows
prop = 1/1000 # Proportion of data to sample
set.seed(60952)
samp <- sample_n(dfs_output_fy18, size = nrow(dfs_output_fy18) * prop) %>%
  # Drop list type
  select_if(
    ~ !is.list(.)
  )

dollars <- samp %>% select(HOUSEHOLD_ID) %>%
  left_join(
    output_fy18_rv %>% filter(HOUSEHOLD_ID %in% samp$HOUSEHOLD_ID)
    , by = c('HOUSEHOLD_ID' = 'HOUSEHOLD_ID')
  ) %>%
  # Fill in NA with 0
  mutate_if(
    is.numeric
    , function(x) ReplaceValues(x)
  )
```

```{r}
# Check that the order is the same
paste(sum(samp$HOUSEHOLD_ID != dollars$HOUSEHOLD_ID), 'mismatched')
```

```{r}
# glmnet LASSO
tmp <- data.frame(fy19_frp = dollars$fy19_frp, samp[, 1:100]) %>%
  na.omit()

m_lasso <- glmnet(
  x = model.matrix(fy19_frp ~ ., data = tmp)[, -1]
  , y = log10(tmp$fy19_frp + 1)
  , family = 'gaussian'
  , alpha = 1
)
```
