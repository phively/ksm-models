---
title: "featuretoolsR demo"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Understand how featuretoolsR, the R implementation of Featuretools ([Python documentation](https://docs.featuretools.com/)) can be used with relational data to reproducibly create derived features. I expect the package can create more features and do it with greater efficiency compared to subquerying/aggregating in SQL.

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(readxl)
library(foreach)
library(featuretoolsR)
```

# Toy data

A small dataset for testing and understanding how some of the transformations work. Taken from the article [Automated Feature Engineering in Python](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219).

```{r, message = FALSE}
clients <- read_csv(file = 'data/demo/clients.csv')
loans <- read_csv(file = 'data/demo/loans.csv')
payments <- read_csv(file = 'data/demo/payments.csv')
```

```{r}
str(clients)
str(loans)
str(payments)
```

Missed should be a boolean.

```{r}
payments <- payments %>%
  mutate(
    missed = missed == 1
  )
```

Create an entityset, i.e. list of related tables.

```{r}
es <- as_entityset(
  clients
  , entity_id = 'clients'
  , index = 'client_id' # Unique identifier
  , time_index = 'joined' # Start date
) %>% add_entity(
  loans
  , entity_id = 'loans'
  , index = 'loan_id'
  , time_index = 'loan_start'
) %>% add_entity(
  payments
  , entity_id = 'payments'
  , make_index = TRUE # Need to create an index because one does not exist
  , index = 'payment_id'
  , time_index = 'payment_date'
)
```

```{r}
print(es)
```

Define relationships between the tables.

```{r}
es <- es %>%
  # Loans are associated with clients
  add_relationship(
    parent_set = 'clients'
    , parent_idx = 'client_id'
    , child_set = 'loans'
    , child_idx = 'client_id'
  ) %>%
  # Payments are associated with loans
  add_relationship(
    parent_set = 'loans'
    , parent_idx = 'loan_id'
    , child_set = 'payments'
    , child_idx = 'loan_id'
  )
```

```{r}
print(es)
```

## DFS primitives

Try out some transformations/aggregations. The built-in primitives:

```{r, rows.print = 20}
list_primitives()
```

Testing some of them out.

```{r}
features <- es %>%
  dfs(
    target_entity = 'clients'
    , agg_primitives = c('mean', 'max', 'count', 'percent_true', 'last')
    , trans_primitives = c('year', 'month', 'is_weekend', 'subtract_numeric', 'divide_numeric', 'time_since_previous')
    , max_depth = 2 # Include up to n levels of feature stacking (2 here)
  )
```

## Results

Print the names of all the features.

```{r}
print_features <- function(fs) {
    foreach(
      i = 1:length(fs[[2]]) # Iterate through each feature
      , .combine = c # Use c() to combine the results below
      ) %do% {
        # Convert each list element to a string
        fs[[2]][[i]] %>% as.character()
    } %>%
    # Print features as data frame rows
    data.frame(feature = ., stringsAsFactors = FALSE)
}
```
```{r, rows.print = 20}
print_features(features)
```

That's quite a lot of features generated by deep feature synthesis for not much work. Here's what some of that data looks like.

```{r}
# Add client_id back as first column
data.frame(client_id = rownames(features[[1]]), features[[1]]) %>%
  write_csv(path = 'data/demo/feature_output.csv')

# Look at some of the features data
features[[1]] %>% str()
```

And all of that took about 2 seconds. Not bad!

```{r}
# Cleanup
remove(clients, es, features, loans, payments)
```


# Alumni data

Now let's try testing it it on raw alumni and committee data.

```{r}
degree <- read_xlsx(
  path = 'data/2020-02-21 entity committee test data.xlsx'
  , sheet = 1
  , guess_max = 1E6
)
committee <- read_xlsx(
  path = 'data/2020-02-21 entity committee test data.xlsx'
  , sheet = 2
  , guess_max = 1E6
)
```

Some data cleanup.

```{r}
degree <- degree %>%
  # Factors
  mutate(
    RECORD_STATUS_CODE = factor(RECORD_STATUS_CODE)
    , PROGRAM = factor(PROGRAM)
    , PROGRAM_GROUP = factor(PROGRAM_GROUP)
  ) %>%
  # Numerics
  mutate_at(
    vars(ends_with('_YEAR'))
    , list(as.numeric)
  )

committee <- committee %>%
  mutate(
    COMMITTEE_DESC = factor(COMMITTEE_DESC)
    , COMMITTEE_ROLE = factor(COMMITTEE_ROLE)
  )
```


Create an entity set, i.e. list of related tables.

```{r}
es <- as_entityset(
  degree # data frame
  , entity_id = 'degree' # name of entity
  , index = 'ID_NUMBER' # unique identifier
  , id = 'CATracks' # name of entity set
) %>% add_entity(
  committee
  , entity_id = 'committee'
  , make_index = TRUE
  , index = 'committee_idx'
  , time_index = 'START_DT_CALC' # when the record can first be used
)
```

Define relationships between the entities.

```{r}
es <- es %>%
  add_relationship(
    parent_set = 'degree'
    , child_set = 'committee'
    , parent_idx = 'ID_NUMBER'
    , child_idx = 'ID_NUMBER'
  )
```

Examine the results.

```{r}
print(es)
```

# DFS results

Deep feature synthesis (DFS) on the alumni data set.

```{r}
t0 <- Sys.time()
features <- es %>% dfs(
  target_entity = 'degree'
  , agg_primitives = c('count', 'sum', 'std', 'last')
  , trans_primitives = c('month', 'year')
  , max_depth = 2
)
t1 <- Sys.time()
```

```{r, rows.print = 20}
print_features(features)
```

```{r}
t1 - t0
```

`r as.numeric(t1 - t0) %>% round() %>% I()` seconds for `r {nrow(committee) + nrow(degree)} %>% I()` rows of data isn't bad. Certainly beats trying to do everything by hand.

Obviously cleanup is needed, as most of these features are pretty silly. The xsequence is essentially arbitrary and should be removed from the data frame in any case. Additionally, it would be useful to calculate things like count of months between start and stop date, distinct roles, most common role (mode), etc.