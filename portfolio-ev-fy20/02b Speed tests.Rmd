---
title: "2b Speed tests"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Determine featuretoolsR scaling with RAM and cores. Does data chunking make a difference or not?

Background: I tried running the dfs algorithm on the entire 80,000+ row data file and it choked at about 77% after about 40 hours and trying to commit over 110 GB (!) of RAM. Clearly need to try something else.

# Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(wranglR)
library(readxl)
library(doSNOW)
library(foreach)
library(reticulate)
library(nanotime)
library(featuretoolsR)

source(file = 'code/featuretools helper functions.R')
```

# Load and check data

```{r, warning = FALSE}
data_dt <- '2020-04-08'
source(file = 'code/data xlsx import.R')
source(file = 'code/data cleanup.R')
source(file = 'code/data validation.R')
```

# Set up Python session

```{python}
import featuretools as ft
```

# Testing with 1% of the data

The first approach I can think of to increasing efficiency is pre-filtering the tables passed to dfs. For example, say there are household_ids in some of the tables not present in the master table. I don't know whether the program is 

```{r}
# Chunk the data into n (approximately) equal-sized bins
catracks_chunked <- chunk_datalist(catracks, master_table_name = 'households', master_idx_name = 'HOUSEHOLD_ID', chunks = 100, seed = 123)
```

Look at the main table from the first chunk.

```{r}
catracks_chunked[[1]][[1]] %>% summary()
```

```{r}
chunk1 <- entityset_create(
  entityset_name = 'chunk1'
  , datalist = catracks_chunked[[1]]
  , cutoff_dt = ymd('20180831')
  , master_entity = 'households'
  , master_idx = 'HOUSEHOLD_ID'
  , debug = TRUE
)

chunk1$add_interesting_values(max_values = 6L)
```

```{r, cache = TRUE}
t1_0 <- Sys.time()
dfs_test <- chunk1 %>%
  dfs(
    target_entity = 'households'
    , agg_primitives = c('count', 'sum', 'std', 'mean', 'max', 'min', 'median', 'first', 'last', 'percent_true')
    , trans_primitives = c('cum_sum', 'cum_max', 'month', 'year', 'subtract_numeric', 'divide_numeric', 'time_since_previous')
    , max_depth = 2
    , verbose = TRUE
  )
t1_1 <- Sys.time()
```

```{r}
t1_1 - t1_0
```

So 1/100 of the datafile only took a few minutes, after prefiltering all the tables. Looks like the maximum RAM committed was 2.9GB. This is promising for thinking about a parallelized workflow.

# Testing 3% of the data in parallel

```{r}
# 3 cores
c3 <- makeCluster(3, outfile = 'logs/c3.txt')
registerDoSNOW(c3)
```

```{r}
# Functions to show progress
show_progress <- function(n, lim) {
  setTxtProgressBar(
    pb = txtProgressBar(max = lim, style = 3)
    , value = n
  )
}
```

```{r, cache = TRUE}
t3_0 <- Sys.time()

chunks <- 3

dfs_test_par3 <- foreach(
  i = 1:chunks
  , .combine = list
  , .multicombine = TRUE
  , .packages = c('tidyverse', 'featuretoolsR', 'lubridate', 'reticulate')
  , .options.snow = list(progress = function(n) {show_progress(n = n, lim = chunks)})
) %dopar% {
  # Extract current dataset
  chunk <- entityset_create(
    entityset_name = 'chunk'
    , datalist = catracks_chunked[[i]]
    , cutoff_dt = ymd('20180831')
    , master_entity = 'households'
  )
  chunk$add_interesting_values(max_values = 6L)
  output <- chunk %>% 
    dfs(
      target_entity = 'households'
      , agg_primitives = c('count')
      , trans_primitives = c('month')
      # , agg_primitives = c('count', 'sum', 'std', 'mean', 'max', 'min', 'median', 'first', 'last', 'percent_true')
      # , trans_primitives = c('cum_sum', 'cum_max', 'month', 'year', 'subtract_numeric', 'divide_numeric', 'time_since_previous')
      , max_depth = 2
      , verbose = TRUE
    )
  return(output)
}
t3_1 <- Sys.time()
```

```{r}
t3_1 - t3_0
```

