---
title: "09 Dollar threshold model"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
---

# Goal

Explore whether using a $\text{giving} \geq g$ dollar threshold improves donor identification compared to no threshold, i.e.

$$ E \left( \text{giving, g level donor | covariates} \right) = E \left(\text{giving | g level donor, covariates} \right) P \left(\text{g level donor | covariates} \right) $$

# Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(reshape2)
library(gridExtra)
library(splines)
library(lubridate)
library(wranglR)
library(Boruta)
library(foreach)
library(doParallel)
library(glmnet)
library(glmnetUtils)
library(mgcv)

# Functions adapted from previous analysis steps
source('code/functions.R')

# Visualization functions adapted fron previous analysis steps
source('code/functions_viz.R')

# Set number of available CPU cores
registerDoParallel(detectCores() - 2)
```
```{r}
# Parameters
train_fy <- 2016
filepath <- 'data/2018-11-30 point-in-time data.xlsx'
sheetname <- 'Select point_in_time_model'

# Import data
source('code/generate-pit-data.R')

# Run data generation functions
giving.threshold = 1E4
modeling.data <- generate_pit_data(filepath, sheetname) %>%
  generate_additional_predictors(giving.threshold = giving.threshold)
```
```{r}
# Withhold 10% of data as test set
xv <- KFoldXVal(modeling.data, k = 2, prop = .1, seed = 6988432)
holdoutdat <- modeling.data[xv[[1]], ]
traindat <- modeling.data[xv[[2]], ]
remove(xv)
```

Setting $g =$ `r giving.threshold %>% scales::dollar() %>% I()` yields `r modeling.data %>% filter(rv.gave == TRUE) %>% nrow()` donors.

# Classification model

## Variable selection

```{r}
# Include all donors
samp <- modeling.data %>% filter(rv.gave == TRUE)
# Sample rows of nondonors
prop = 1/20 # Proportion of nondonors to sample
set.seed(378055)
samp <- rbind(
  samp
  , sample_n(modeling.data %>% filter(rv.gave == FALSE)
             , size = nrow(modeling.data %>% filter(rv.gave == FALSE)) * prop)
)
# Run Boruta algorithm
rf.vars <- Boruta(
  y = as.numeric(samp$rv.gave)
  , x = samp %>% select(-rv.amt, -rv.gave, -ID_NUMBER, -HOUSEHOLD_ID, -INSTITUTIONAL_SUFFIX, -DEGREES_CONCAT)
  , seed = 8906
)
```
```{r, fig.width = 8, fig.height = 20}
rf.vars %>% Borutadata() %>% Borutaplotter()
```

```{r}
glm.recommended.vars <- rf.vars %>% TentativeRoughFix()
print(glm.recommended.vars)
```

## Model tuning

```{r}
gam_p <- gam(
  rv.gave ~
    s(CASH_PFY1) +
    s(CASH_PFY2) +
    s(CASH_PFY3) +
    s(CASH_PFY4) +
    s(CASH_PFY5) +
    s(NGC_PFY1) +
    s(NGC_PFY2) +
    s(NGC_PFY3) +
    s(NGC_PFY4) +
    s(NGC_PFY5) +
    s(VELOCITY3_LIN_CASH) +
    CRU_GIVING_SEGMENT +
    s(GIFTS_OUTRIGHTS_PAYMENTS) +
    s(GIVING_MAX_CASH_AMT) +
    s(GIVING_MAX_PLEDGE_AMT) +
    s(GIVING_CASH_TOTAL) +
    s(GIFTS_FYS_SUPPORTED) +
    s(VISITS_5FY) +
    s(COMMITTEE_NU_YEARS) +
    s(GIVING_MAX_CASH_YR) +
    s(RECORD_YR) +
    KSM_PROSPECT +
    s(UOR_LOWER_BOUND) +
    s(MONTHS_ASSIGNED) +
    s(KSM_EVENTS_YRS) +
    s(COMMITTEES_CFY) +
    BUS_IS_EMPLOYED
  , data = traindat
  , family = 'binomial'
  , control = list(nthreads = 10)
)
```
```{r}
summary(gam_p)
```

```{r}
theta1 <- sum(traindat$rv.gave) / nrow(traindat)
gam_stats <- conf_matrix(gam_p, newdata = holdoutdat, rv = 'rv.gave', threshold = theta1)
gam_stats5 <- conf_matrix(gam_p, newdata = holdoutdat, rv = 'rv.gave', threshold = 0.5)
```
```{r}
data.frame(
  model = c('gam', 'gam .5')
  , error = c(gam_stats$error, gam_stats5$error)
  , precision = c(gam_stats$precision, gam_stats5$precision)
  , sensitivity = c(gam_stats$sensitivity, gam_stats5$sensitivity)
  , F1 = c(gam_stats$F1_score, gam_stats5$F1_score)
)
```

Of course, using threshold $\hat{\theta} =$ `r theta1 %>% round(4)` results in much higher sensitivity at the cost of false positives.

```{r}
data.frame(
  class = as.numeric(holdoutdat$rv.gave)
  , prediction = predict(gam_p, newdata = holdoutdat, type = 'response')
) %>%
  ggplot(aes(x = prediction, y = class)) +
  geom_point(aes(color = factor(class))) +
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = 'loess') +
  labs(title = 'Out-of-sample calibration plot with gam smoothers'
       , x = 'predicted probability'
       , y = 'observed probability')
```

The out-of-sample calibration looks so-so.

# Regression model

## Variable selection

```{r}
mg.donors <- traindat %>%
  filter(rv.gave == TRUE) %>%
  select(-rv.gave, -ID_NUMBER, -HOUSEHOLD_ID, -INSTITUTIONAL_SUFFIX, -DEGREES_CONCAT)

# Run Boruta algorithm
rf.vars.lm <- Boruta(
    y = log10(mg.donors$rv.amt)
    , x = mg.donors %>% select(-rv.amt)
    , seed = 3137965
  )
```
```{r, fig.width = 8, fig.height = 20}
rf.vars.lm %>% Borutadata() %>% Borutaplotter()
```

```{r}
lm.recommended.vars <- rf.vars.lm %>% TentativeRoughFix()
print(lm.recommended.vars)
```
```{r}
lm.recommended.vars$finalDecision[which(lm.recommended.vars$finalDecision == 'Confirmed')] %>% names()
```

Again, many of these are collinear.

## Model tuning

```{r}
gam_donors <- gam(
  rv.amt ~
    GIFT_CLUB_BEQUEST_YRS +
    s(GIVING_MAX_CASH_AMT) +
    s(UOR_LOWER_BOUND) +
    GIFT_CLUB_KLC_YRS +
    s(CASH_PFY1) +
    s(CASH_PFY3) +
    s(NGC_PFY1) +
    s(NGC_PFY3) +
    s(NGC_PFY5) +
    s(VELOCITY3_LIN_CASH) +
    s(VISITS_5FY) +
    s(EVENTS_PFY1) +
    KSM_PROSPECT +
    s(MONTHS_ASSIGNED) +
    s(COMMITTEE_NU_DISTINCT) +
    s(PLEDGE_BALANCE)
  , data = mg.donors
  , family = 'gaussian'
)
```
```{r}
summary(gam_donors)
```

The $R^2_\text{adj}$ isn't bad.

## All data model

Additionally, fit a regression model on the entire dataset, not just high-end `r giving.threshold %>% scales::dollar()` donors.

```{r}
gam_all <- gam(
  rv.amt ~
    GIFT_CLUB_BEQUEST_YRS +
    s(GIVING_MAX_CASH_AMT) +
    s(UOR_LOWER_BOUND) +
    GIFT_CLUB_KLC_YRS +
    s(CASH_PFY1) +
    s(CASH_PFY3) +
    s(NGC_PFY1) +
    s(NGC_PFY3) +
    s(NGC_PFY5) +
    s(VELOCITY3_LIN_CASH) +
    s(VISITS_5FY) +
    s(EVENTS_PFY1) +
    KSM_PROSPECT +
    s(MONTHS_ASSIGNED) +
    s(COMMITTEE_NU_DISTINCT) +
    s(PLEDGE_BALANCE)
  , data = traindat
  , family = 'gaussian'
)
```

## Comparison

```{r}
data.frame(
  model = c('gam threshold', 'gam all')
  , adj.r.sq = c(summary(gam_donors)$r.sq, summary(gam_all)$r.sq)
  , insample_mse = c(
    calc_mse(y = gam_donors$model$rv.amt, yhat = gam_donors$fitted)
    , calc_mse(y = gam_all$model$rv.amt, yhat = gam_all$fitted)
  )
  , outsample_mse = c(
    calc_mse(
      y = holdoutdat %>% filter(rv.gave == TRUE) %>% select(rv.amt) %>% unlist()
      , yhat = predict(gam_donors, newdata = holdoutdat %>% filter(rv.gave == TRUE))
    )
    , calc_mse(
      y = holdoutdat %>% select(rv.amt) %>% unlist()
      , yhat = predict(gam_all, newdata = holdoutdat)
    )
  )
  , overall_mse = c(
    calc_mse(y = modeling.data$rv.amt, yhat = predict(gam_donors, newdata = modeling.data))
    , calc_mse(y = modeling.data$rv.amt, yhat = predict(gam_all, newdata = modeling.data))
  )
)
```

In this case, the intercept for the `r giving.threshold %>% scales::dollar()` threshold model leads to a huge overall MSE. However, in the final model this will be reduced because most entities have a very low $P \left(\text{g level donor | covariates} \right)$

# Sorted list

Order entities by having a KSM manager, then by university overall rating, then by evaluation rating, then by total past giving (NGC), then by last yearâ€™s giving (NGC), and finally by alpha ordering.

```{r}
eval_giving <- modeling.data %>%
  arrange(
    desc(KSM_GOS_FLAG)
    , desc(UOR_LOWER_BOUND)
    , desc(EVALUATION_LOWER_BOUND)
    , desc(GIVING_NGC_TOTAL)
    , desc(NGC_PFY1)
    , REPORT_NAME
  ) %>% select(
    ID_NUMBER
    , KSM_GOS_FLAG
    , UOR_LOWER_BOUND
    , EVALUATION_LOWER_BOUND
    , GIVING_NGC_TOTAL
    , NGC_PFY1
    , REPORT_NAME
    , rv.amt
  ) %>% mutate(
    fitted = nrow(modeling.data) - row_number()
    , giving = log10plus1(rv.amt, inverse = TRUE)
  )
```


# Dollars under the curve

```{r}
predictions <- data.frame(
  truth = modeling.data$rv.amt
  , prob.mod = predict(gam_p, newdata = modeling.data, type = 'response')
  , reg.mod = predict(gam_donors, newdata = modeling.data)
  , allreg.mod = predict(gam_all, newdata = modeling.data)
) %>% mutate(
  ev.thresh = prob.mod * reg.mod
  , ev.all = prob.mod * allreg.mod
)
```
```{r}
# Create lines for plotting
duc_data <- rbind(
  data.frame(model = 'probability', duc_data_gen(predictions$prob.mod, predictions$truth))
  , data.frame(model = 'reg thresh', duc_data_gen(predictions$reg.mod, predictions$truth))
  , data.frame(model = 'reg all', duc_data_gen(predictions$allreg.mod, predictions$truth))
  , data.frame(model = 'ev thresh', duc_data_gen(predictions$ev.thresh, predictions$truth))
  , data.frame(model = 'ev all', duc_data_gen(predictions$ev.all, predictions$truth))
  , data.frame(model = 'eval giving', duc_data_gen(eval_giving$fitted, eval_giving$rv.amt))
)
```

```{r}
duc_data %>%
  ggplot(aes(x = pct, y = logdollars, color = model)) +
  geom_line(size = 1, alpha = .8) +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed', col = 'black') +
  scale_x_continuous(breaks = seq(0, 1, by = .1)
                     , labels = seq(0, 1, by = .1) %>% scales::percent()
                     , expand = c(0, 0)) +
  scale_y_continuous(breaks = seq(0, 1, by = .1)
                     , labels = seq(0, 1, by = .1) %>% scales::percent()
                     , expand = c(0, 0)) +
  coord_equal() +
  labs(
    title = 'Log dollars captured by % of data file examined'
    , x = '% of data file'
    , y = '% of log dollars'
  )
```

```{r}
duc_data %>%
  ggplot(aes(x = pct, y = dollars, color = model)) +
  geom_line(size = 1, alpha = .8) +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed', col = 'black') +
  scale_x_continuous(breaks = seq(0, 1, by = .1)
                     , labels = seq(0, 1, by = .1) %>% scales::percent()
                     , expand = c(0, 0)) +
  scale_y_continuous(breaks = seq(0, 1, by = .1)
                     , labels = seq(0, 1, by = .1) %>% scales::percent()
                     , expand = c(0, 0)) +
  coord_equal() +
  labs(
    title = 'Total dollars captured by % of data file examined'
    , x = '% of data file'
    , y = '% of dollars'
  )
```

The thresholded probability model greatly outperforms the thresholded regression model. However, the regression model fit to the entire dataset outperforms everything else handily. Apparently, fitting a model to just high-end donors results in greatly degraded performance compared to looking at all donors and gifts at any dollar level.