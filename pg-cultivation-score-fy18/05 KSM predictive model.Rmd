---
title: "05 KSM predictive model"
output:
  html_notebook:
    code_folding: hide
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Build a basic campaign prioritization model using all relevant variables extracted from the database and identified in previous work.

# Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(reshape2)
library(gridExtra)
library(splines)
library(foreach)
library(lubridate)
library(wranglR)
library(Boruta)

# Functions adapted from previous analysis steps
source('code/functions.R')

# Visualization functions adapted fron previous analysis steps
source('code/functions_viz.R')
```

# KSM model goals

The overarching goal is to predict giving over the final two years of the campaign. Ideally, I'd want to find expected future value, not just difference from expected value today. Consider the following:

$$ E \left( \text{giving, donor | covariates} \right) = E \left(\text{giving | donor, covariates} \right) P \left(\text{donor | covariates} \right) $$

Estimate the expected future value as the product of an expected value and a probability. This can also be thought of as separate capacity and affinity models, and should give more useful estimates than $E\left( \text{giving | covariates} \right)$, which is left-censored by \$0.

It'll be informative seeing what features are more or less important at each stage of the two-step procedure, though I expect overall accuracy to suffer somewhat. Down the road it would be interesting to compare this to other methods, like trees and boosting.

# KSM model variables

The target variable is the sum of new gifts and commitments from 9/1/16 to 8/31/18 (FY17-18), given the state of the database on 8/31/16 (FY16).

As a general principle, point-in-time data is derived from entered date ranges where possible. Where dates are missing, it will be based upon the date added or date modified audit trail for each field, as suitable. The following data types received this treatment:

  * All dollar amounts
  * All giving behavior counts and years
  * Student/alumni status
  * All prospect assignments and ratings
  * All contact information and indicators
  * Employment
  * Visits and outreach
  * Engagement counts

This is implemented by [this SQL code](https://github.com/phively/ksm-models/blob/master/pg-cultivation-score-fy18/code/ksm-point-in-time-data-pull.sql).

```{r}
# Parameters
train_fy <- 2016
filepath <- 'data/2018-11-29 point-in-time data.xlsx'
sheetname <- 'Select point_in_time_model'

# Import data
source('code/generate-pit-data.R')

# Run data generation function
modeling.data <- generate_pit_data(filepath, sheetname)

# Create response variables
modeling.data <- modeling.data %>% mutate(
  rv.amt = NGC_TARGET_FY2 + NGC_TARGET_FY1
  , rv.gave = rv.amt > 0
) %>% select(
  # Drop future data
  -NGC_TARGET_FY2
  , -NGC_TARGET_FY1
  , -CASH_TARGET_FY2
  , -CASH_TARGET_FY1
  , -AF_TARGET_FY2
  , -AF_TARGET_FY1
  , -CRU_TARGET_FY2
  , -CRU_TARGET_FY1
) %>% filter(
  # Drop entities whose RECORD_YR is after the training year
  RECORD_YR <= train_fy
)
```

# Probability model

Logistic regression has been the workhorse of fundraising models for years. Some special considerations for this application:

  * Minimizing predictive error, e.g. finding the model $\text{argmin}_m \sum_i \left[ y_i - \widehat{m}_x(x_i) \right]^2$, on in-sample data is the *wrong* metric!
  * Focus on identifying as many current prospects as possible (minimizing type II error); type I is acceptable as these become new prospects.
  * Avoid overfitting the training data. Techniques like [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) are highly recommended.
  * Avoid [endogenous](https://en.wikipedia.org/wiki/Endogeneity_(econometrics)) variables; in this context, that means those that are causally associated with the outcome being measured, e.g. don't use `Lifetime.Giving` as a predictor if the response variable is `Largest.Gift`.

I have [previously found](https://github.com/phively/ksm-models/tree/master/af-10k-fy17) that penalized logistic regression, such as implemented in R by the glmnet package, works better than standard logistic regression, so that's the technique that I'll use here.

Here, the response variable is:

$$ Y_i = I \left( \text{FY18Giving}_i + \text{FY17Giving}_i > 0  \right) $$

## Variable selection

I like computing random forest variable importance, e.g. [Sauve & Tuleau-Malot (2014)](https://hal-unice.archives-ouvertes.fr/hal-00551375/document), to pre-screen variables. Define variable importance in a random forest as the change in MSE when permuting a given observation vector. One nice feature is that highly correlated variables should be similarly important.


```{r}
# Sample rows
prop = 1/20 # Proportion of data to sample
set.seed(287092)
samp <- sample_n(modeling.data, size = nrow(modeling.data) * prop)

# Run Boruta algorithm
(rf.vars <- Boruta(
    y = as.numeric(samp$rv.gave)
    , x = samp %>% select(-rv.amt, -rv.gave)
    , seed = 5993207
  )
)
```

Save the results.

```{r}
save(rf.vars, file = 'data/rf.vars.Rdata')
```


Plot the results.

```{r, fig.width = 8, fig.height = 20}
rf.vars %>% Borutadata() %>% Borutaplotter()
```

Basically, the algorithm creates dummy "shadow" variables, which are permuted versions of the explanatory variables appearing above, and random forests are fit on both the real and dummy variables. Intuitively, if replacing a variable with a randomly permuted version of itself does not reduce the random forest classifier's accuracy, then the variable should not be included in a final model and can be discarded.

Recall that the response variable is making a new gift or commitment at any level within the next two years. From past experience, I know that most donations are outright gifts, under $1,000, and to an annual giving allocation. So the following is not too surprising:

  * Past giving is the best predictor of future giving
  * More recent giving behavior is more predictive than less recent giving behavior
  * Other engagement indicators (e.g. events, committees) are predictive in aggregate
  * Cash is more predictive than new gifts & commitments
  * Having an active home email address and home phone number is predictive

I found these more surprising:

  * Degree program is not predictive
  * Having an active mailing address is not predictive
  * Prospect indicators (is a prospect, number of visits, rating, etc.) are not predictive
  * ID numbers are predictive, but other personal identifiers like name are not -- presumably because of the ID number/age correlation?

```{r}
(recommended.vars <- TentativeRoughFix(rf.vars))
```

```{r, fig.width = 16, fig.height = 16}
# Check variable correlations
recommended_vars <- recommended.vars$finalDecision[
  which(recommended.vars$finalDecision == 'Confirmed')] %>% names()
numeric_vars <- modeling.data %>%
  select(recommended_vars) %>%
  select(-ID_NUMBER, -HOUSEHOLD_ID) %>%
  select_if(is.numeric)
numeric_vars %>% plot_corrs(textsize = 2)
```

This is the correlation matrix for all `r numeric_vars %>% ncol() %>% I()` numeric variables confirmed important by the algorithm.

  * AF, cash, and CRU are not as highly correlated as I would've expected. However, AF and CRU are moderately highly correlated, and CRU is more correlated with itself than AF, so consider using AF and cash only.
  * Count of gifts and payments, count of cash gifts, count of FYs supported, and count of allocations supported are all highly correlated. Consider dropping some of them.

## Cross-validation

Create modeling data file.

```{r}
mdat <- modeling.data %>% select(rv.gave, recommended_vars) %>%
  select(
    -VELOCITY3_NGC, -VELOCITY_BINS_CASH, -VELOCITY_BINS_NGC, -VELOCITY3_LIN_NGC
    , -CRU_PFY1, -CRU_PFY2, -CRU_PFY3, -CRU_PFY4, -CRU_PFY5
    , -BIRTH_DT, -ID_NUMBER, -HOUSEHOLD_ID, -DEGREES_CONCAT
    , -GIVING_NGC_TOTAL, -LOYAL_5_PCT_CASH, -UPGRADE3_NGC, -CRU_STATUS
  )
```




