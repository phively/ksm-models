---
title: "05 KSM predictive model"
output:
  html_notebook:
    code_folding: hide
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Build a basic campaign prioritization model using all relevant variables extracted from the database and identified in previous work.

# Setup

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(reshape2)
library(gridExtra)
library(splines)
library(lubridate)
library(wranglR)
library(Boruta)
library(foreach)
library(doParallel)
library(glmnet)
library(glmnetUtils)

# Functions adapted from previous analysis steps
source('code/functions.R')

# Visualization functions adapted fron previous analysis steps
source('code/functions_viz.R')

# Set number of available CPU cores
registerDoParallel(detectCores() - 1)
```

# KSM model goals

The overarching goal is to predict giving over the final two years of the campaign. Ideally, I'd want to find expected future value, not just difference from expected value today. Consider the following:

$$ E \left( \text{giving, donor | covariates} \right) = E \left(\text{giving | donor, covariates} \right) P \left(\text{donor | covariates} \right) $$

Estimate the expected future value as the product of an expected value and a probability. This can also be thought of as separate capacity and affinity models, and should give more useful estimates than $E\left( \text{giving | covariates} \right)$, which is left-censored by \$0.

It'll be informative seeing what features are more or less important at each stage of the two-step procedure, though I expect overall accuracy to suffer somewhat. Down the road it would be interesting to compare this to other methods, like trees and boosting.

# KSM model variables

The target variable is the sum of new gifts and commitments from 9/1/16 to 8/31/18 (FY17-18), given the state of the database on 8/31/16 (FY16).

As a general principle, point-in-time data is derived from entered date ranges where possible. Where dates are missing, it will be based upon the date added or date modified audit trail for each field, as suitable. The following data types received this treatment:

  * All dollar amounts
  * All giving behavior counts and years
  * Student/alumni status
  * All prospect assignments and ratings
  * All contact information and indicators
  * Employment
  * Visits and outreach
  * Engagement counts

This is implemented by [this SQL code](https://github.com/phively/ksm-models/blob/master/pg-cultivation-score-fy18/code/ksm-point-in-time-data-pull.sql).

```{r}
# Parameters
train_fy <- 2016
filepath <- 'data/2018-11-30 point-in-time data.xlsx'
sheetname <- 'Select point_in_time_model'

# Import data
source('code/generate-pit-data.R')

# Run data generation function
modeling.data <- generate_pit_data(filepath, sheetname)

# Create response variables
modeling.data <- modeling.data %>% mutate(
  rv.amt = NGC_TARGET_FY2 + NGC_TARGET_FY1
  , rv.gave = rv.amt > 0
) %>% select(
  # Drop future data
  -NGC_TARGET_FY2
  , -NGC_TARGET_FY1
  , -CASH_TARGET_FY2
  , -CASH_TARGET_FY1
  , -PLEDGE_TARGET_FY2
  , -PLEDGE_TARGET_FY1
  , -AF_TARGET_FY2
  , -AF_TARGET_FY1
  , -CRU_TARGET_FY2
  , -CRU_TARGET_FY1
) %>% filter(
  # Drop entities whose RECORD_YR is after the training year
  RECORD_YR <= train_fy
)
```

# Probability model

Logistic regression has been the workhorse of fundraising models for years. Some special considerations for this application:

  * Minimizing predictive error, e.g. finding the model $\text{argmin}_m \sum_i \left[ y_i - \widehat{m}_x(x_i) \right]^2$, on in-sample data is the *wrong* metric!
  * Focus on identifying as many current prospects as possible (minimizing type II error); type I is acceptable as these become new prospects.
  * Avoid overfitting the training data. Techniques like [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) are highly recommended.
  * Avoid [endogenous](https://en.wikipedia.org/wiki/Endogeneity_(econometrics)) variables; in this context, that means those that are causally associated with the outcome being measured, e.g. don't use `Lifetime.Giving` as a predictor if the response variable is `Largest.Gift`.

I have [previously found](https://github.com/phively/ksm-models/tree/master/af-10k-fy17) that penalized logistic regression, such as implemented in R by the glmnet package, works better than standard logistic regression, so that's the technique that I'll use here.

Here, the response variable is:

$$ Y_i = I \left( \text{FY18Giving}_i + \text{FY17Giving}_i > 0  \right) $$

## Variable selection

I like computing random forest variable importance, e.g. [Sauve & Tuleau-Malot (2014)](https://hal-unice.archives-ouvertes.fr/hal-00551375/document), to pre-screen variables. Define variable importance in a random forest as the change in MSE when permuting a given observation vector. One nice feature is that highly correlated variables should be similarly important.


```{r, cache = TRUE}
# Sample rows
prop = 1/5 # Proportion of data to sample
set.seed(287092)
samp <- sample_n(modeling.data, size = nrow(modeling.data) * prop)

# Run Boruta algorithm
rf.vars <- Boruta(
    y = as.numeric(samp$rv.gave)
    , x = samp %>% select(-rv.amt, -rv.gave)
    , seed = 5993207
  )
```
```{r}
rf.vars %>% print()
```


Save the results.

```{r}
save(rf.vars, file = 'data/rf.vars.Rdata')
```


Plot the results.

```{r, fig.width = 8, fig.height = 20}
(pmod_plot <- rf.vars %>% Borutadata() %>% Borutaplotter())
```

Basically, the algorithm creates dummy "shadow" variables, which are permuted versions of the explanatory variables appearing above, and random forests are fit on both the real and dummy variables. Intuitively, if replacing a variable with a randomly permuted version of itself does not reduce the random forest classifier's accuracy, then the variable should not be included in a final model and can be discarded.

Recall that the response variable is making a new gift or commitment at any level within the next two years. From past experience, I know that most donations are outright gifts, under $1,000, and to an annual giving allocation. So the following is not too surprising:

  * Past giving is the best predictor of future giving
  * More recent giving behavior is more predictive than less recent giving behavior
  * Other engagement indicators (e.g. events, committees) are predictive in aggregate
  * Cash is more predictive than new gifts & commitments
  * Having active home contact information is predictive

I found these more surprising:

  * Having business contact information is not predictive
  * Prospect indicators (is a prospect, number of visits, rating, etc.) are only weakly predictive
  * ID numbers are predictive, but other personal identifiers like name are not -- presumably because of the ID number/age correlation?
  * KSM-specific engagement is more predictive than NU engagement

```{r}
(recommended.vars <- TentativeRoughFix(rf.vars))
```

```{r, fig.width = 16, fig.height = 16}
# Check variable correlations
recommended_vars <- recommended.vars$finalDecision[
  which(recommended.vars$finalDecision == 'Confirmed')] %>% names()
numeric_vars <- modeling.data %>%
  select(recommended_vars) %>%
  select(-ID_NUMBER, -HOUSEHOLD_ID) %>%
  select_if(is.numeric)
numeric_vars %>% plot_corrs(textsize = 2)
```

This is the correlation matrix for all `r numeric_vars %>% ncol() %>% I()` numeric variables confirmed important by the algorithm.

  * AF, cash, and CRU are not as highly correlated as I would've expected. However, AF and CRU are moderately highly correlated, AF's definition has changed over time, so consider using CRU and cash only.
  * Count of gifts and payments, count of cash gifts, count of FYs supported, and count of allocations supported are all highly correlated. Consider dropping some of them.

## Cross-validation

Begin by creating the modeling data file.

```{r}
# Data file with variables removed
mdat <- modeling.data %>% select(rv.gave, recommended_vars) %>%
  select(
    -VELOCITY3_NGC, -VELOCITY_BINS_NGC, -VELOCITY_BINS_CASH, -VELOCITY3_LIN_NGC
    , -GIVING_MAX_PLEDGE_YR, -GIVING_MAX_PLEDGE_FY, -CRU_STATUS
    , -NGC_PFY1, -NGC_PFY2, -NGC_PFY3, -NGC_PFY4, -NGC_PFY5
    , -AF_PFY1, -AF_PFY2, -AF_PFY3, -AF_PFY4, -AF_PFY5
    , -GIVING_MAX_CASH_FY, -GIVING_NGC_TOTAL, -UPGRADE3_NGC, -LOYAL_5_PCT_ANY
    , -DEGREES_CONCAT, -BIRTH_DT, -FIRST_KSM_YEAR
    , -ID_NUMBER, -INSTITUTIONAL_SUFFIX # Keep HHID but don't use in modeling
    , -KSM_GOS, -HOUSEHOLD_COUNTRY
    , -KSM_EVENTS_ATTENDED, -EVENTS_ATTENDED
  ) %>% mutate(
    # Create spouse flag
    SPOUSE_ALUM = ifelse(SPOUSE_FIRST_KSM_YEAR > 0, 'TRUE', 'FALSE') %>% factor()
  ) %>% mutate_if(
    # Numeric variables over 1E4 get a log10 transformation
    function(x) {
      ifelse(is.numeric(x), max(x) >= 1E4, FALSE)
    }
    , log10plus1
  )

# Cross-validation settings
folds = 10
reps = 5

# Withhold 10% of data as test set
xv <- KFoldXVal(mdat, k = 2, prop = .1, seed = 4960582)
holdoutdat <- mdat[xv[[1]], ]
traindat <- mdat[xv[[2]], ]
remove(xv)
```

### Recommendations

  * `r folds %>% I()`-fold cross-validation repeated `r reps %>% I()` times
  * Estimate prediction error with out-of-sample classification error
  * $\theta_{1}$ threshold (donors) set to the empirical probability in the cross-validation set
  * Try to preserve continuous variables; reasonable monotonic transformations are fine, but avoid discretization

### Baseline penalized logistic regression

I'll use a penalized ridge regression model as implemented by glmnet. Advantages of shrinkage techniques include automatically controlling for overfitting and collinearity.

```{r, cache = TRUE}
# Store timings
glm_ridge_baseline_timestamps <- list()
# Store model errors
glm_nospline <- list()
# Seed for reproducibility
set.seed(2934223)

# Outer loop (repetitions)
for (rep in 1:reps) {
  # Status report 
  timestamp <- paste('+ Iteration', rep, 'beginning at:', Sys.time())
  print(timestamp)
  glm_ridge_baseline_timestamps <- c(glm_ridge_baseline_timestamps, timestamp)
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  # Inner loop (parallel cross-validation)
  errs_out <- foreach(
    fold = 1:length(xv)
    , .combine = c
    , .packages = c('glmnet', 'glmnetUtils', 'dplyr', 'splines')
  ) %dopar% {
    # Fit temp model, where alpha = 0 is the ridge regression penalty
      tmpmodel <- cv.glmnet(
        rv.gave ~ .
        # Train while withholding some data
        , data = traindat[-xv[[fold]], ] %>% select(-HOUSEHOLD_ID)
        , family = 'binomial'
        , alpha = 0
        , lambda = 2^(-8:5)
      )
    # Prediction threshold
    theta1 <- sum(traindat$rv.gave[-xv[[fold]]] == 1) / nrow(traindat[-xv[[fold]], ])
    # Confusion matrix based on the withheld data
    tmpconfus <- conf_matrix_glmnet(tmpmodel, newdata = traindat[xv[[fold]], ], rv = 'rv.gave', threshold = theta1)
    # Return results
    return(
      list(
        conf_matrix = tmpconfus$conf_matrix
        , conf_matrix_pct = tmpconfus$conf_matrix_pct
        , errors = data.frame(
          reps = rep
          , folds = fold
          , error = tmpconfus$error
          , precision = tmpconfus$precision
          , sensitivity = tmpconfus$sensitivity
          , F1_score = tmpconfus$F1_score
        )
      )
    )
  }
  # Write results to errors data frame
  glm_nospline <- c(glm_nospline, errs_out)
  # Status report
  timestamp <- paste(' -Iteration', rep, 'ending at:   ', Sys.time())
  print(timestamp)
  glm_ridge_baseline_timestamps <- c(glm_ridge_baseline_timestamps, timestamp)
}
```
```{r}
glm_ridge_baseline_timestamps %>% unlist() %>% print()
```

```{r}
# Function to reshape list data
combine_xval <- function(xval_results = list()) {
  # Function to reformat list output into groups
  delister <- function(full_list, first_idx = 1, seq) {
    output <- list()
    idx <- seq(first_idx, length(full_list), by = seq)
    for (i in 1:length(idx)) {
      output <- c(output, full_list[idx[i]])
    }
    return(output)
  }
  # Separate the output into groups of 3
  conf_matrix = delister(xval_results, 1, 3)
  conf_matrix_pct = delister(xval_results, 2, 3)
  errors = delister(xval_results, 3, 3)
  # Turn errors into a data frame
  errors <- foreach(i = 1:length(errors), .combine = rbind) %do% {
    return(errors[[i]])
  } %>% data.frame()
  # Return organized list
  return(
    list(
      conf_matrix = conf_matrix
      , conf_matrix_pct = conf_matrix_pct
      , errors = errors
    )
  )
}
```

```{r}
# Save results
glm_ridge_baseline_results <- combine_xval(glm_nospline)
glm_ridge_baseline_model <- cv.glmnet(
        rv.gave ~ .
        , data = traindat %>% select(-HOUSEHOLD_ID)
        , family = 'binomial'
        , alpha = 0
      )
save(
  glm_nospline
  , glm_ridge_baseline_model
  , glm_ridge_baseline_results
  , glm_ridge_baseline_timestamps
  , file = 'data/glm_ridge_baseline.Rdata'
)
```

```{r}
grid.arrange(
    histogrammer(glm_ridge_baseline_results$errors, 'error', h = .0005, fill = 'pink')
  , histogrammer(glm_ridge_baseline_results$errors, 'precision', h = .005, fill = 'cyan')
  , histogrammer(glm_ridge_baseline_results$errors, 'sensitivity', h = .005, fill = 'green')
)
```

Let TP, TN, FP, FN refer to true positives, true negatives, false positives, and false negatives respectively.

$$ \text{error} = \frac{FP + FN}{n}$$
$$ \text{precision} = \frac{TP}{TP + FP}$$
$$ \text{sensitivity} = \frac{TP}{TP + FN}$$

Compared to the AF $10K model, this has higher error due to the decreased sensitivity, but much higher precision.

The metrics to beat so far:

```{r}
(
glm_baseline_err <- data.frame(
  glm_ridge_baseline = glm_ridge_baseline_results$errors %>%
    select(-reps, -folds) %>%
    colMeans()
)
)
```

### Standard logistic regression

Consider a standard logistic regression model to get a better sense of the explanatory variables.

```{r}
glm_standard <- glm(
  rv.gave ~ .
  , data = traindat %>% select(-HOUSEHOLD_ID) %>%
    select(-RECORD_STATUS_CODE) # Results in separation if included
  , family = 'binomial'
)
```
```{r}
summary(glm_standard)
```
```{r, fig.width = 20, fig.height = 20}
summary(glm_standard, corr = TRUE)$correlation %>%
  data.frame() %>%
  plot_corrs()
```

Pretty eyewatering. Look at the term plots.

```{r}
termplot(glm_standard)
```

Definitely keep:

  * Program group looks very interesting
  * Pref addr type code could be interesting
  * CRU giving segment looks very interesting too
  * Spouse alum or spouse KSM year, not both
  
Definitely drop:

  * Has Home Email
  * Gifts credit card
  * Gift clubs fields
  * KSM GOs flag
  * Evaluation and UOR
  * KSM prospect flag
  * KSM Events Reunions
  * Giving max pledge mo

Needs transformation:

  * Giving first year
  * Months assigned
  * Events prev 3 FY
  * Events CFY
  * Athletics ticket last
  * Record yr
  * Max cash year

Duplicative:

  * Giving first year pledge amt
  * Giving max cash amt
  * Giving AF total
  * Gifts outrights payments
  * CRU PFY1 through 5
  * Committees CFY and PFY 1-3
  * Events yrs and KSM events yrs
  * Velocity3 cash
  * Spouse first KSM year

```{r}
glm_standard <- glm_standard %>% update(
  data = traindat %>% select(
    -HOUSEHOLD_ID
    , -RECORD_STATUS_CODE
    # Drop
    , -HAS_HOME_EMAIL
    , -GIFTS_CREDIT_CARD
    , -contains('GIFT_CLUB')
    , -KSM_GOS_FLAG
    , -EVALUATION_LOWER_BOUND
    , -UOR_LOWER_BOUND
    , -KSM_PROSPECT
    , -KSM_EVENTS_REUNIONS
    , -GIVING_MAX_PLEDGE_MO
    # Duplicative
    , -GIVING_FIRST_YEAR_PLEDGE_AMT
    , -GIVING_MAX_CASH_AMT
    , -GIVING_AF_TOTAL
    , -GIFTS_OUTRIGHTS_PAYMENTS
    , -contains('CRU_PFY')
    , -contains('COMMITTEES_')
    , -contains('EVENTS_YRS')
    , -VELOCITY3_CASH
    , -SPOUSE_FIRST_KSM_YEAR
  )
)
```
```{r}
summary(glm_standard)
```

The AIC is higher, but comparable. 

### Logistic regression with splines

```{r}
dfs <- 4
```

Now introduce splines on the numeric variables, arbitrarily setting df = `r dfs %>% I()`.

```{r}
glm_st_splines <- glm(
  rv.gave ~
    PROGRAM_GROUP +
    PREF_ADDR_TYPE_CODE +
    HOUSEHOLD_CONTINENT +
    BUS_IS_EMPLOYED +
    HAS_HOME_ADDR +
    HAS_HOME_PHONE +
    ns(YEARS_SINCE_FIRST_GIFT, df = dfs) +
    ns(GIVING_FIRST_YEAR_CASH_AMT, df = dfs) +
    ns(GIVING_MAX_PLEDGE_AMT, df = dfs) +
    ns(GIVING_CASH_TOTAL, df = dfs) +
    ns(GIVING_PLEDGE_TOTAL, df = dfs) +
    ns(GIVING_CRU_TOTAL, df = dfs) +
    ns(GIFTS_ALLOCS_SUPPORTED, df = dfs) +
    ns(GIFTS_FYS_SUPPORTED, df = dfs) +
    ns(GIFTS_CASH, df = dfs) +
    ns(GIFTS_PLEDGES, df = dfs) +
    ns(CASH_PFY1, df = dfs) +
    ns(CASH_PFY2, df = dfs) +
    ns(CASH_PFY3, df = dfs) +
    ns(CASH_PFY4, df = dfs) +
    ns(CASH_PFY5, df = dfs) +
    CRU_GIVING_SEGMENT +
    ns(EVALUATION_LOWER_BOUND, df = dfs) +
    ns(UOR_LOWER_BOUND, df = dfs) +
    ns(MONTHS_ASSIGNED, df = dfs) +
    ns(COMMITTEE_NU_DISTINCT, df = dfs) +
    ns(COMMITTEE_NU_YEARS, df = dfs) +
    ns(COMMITTEE_KSM_DISTINCT, df = dfs) +
    ns(EVENTS_PREV_3_FY, df = dfs) +
    ns(EVENTS_CFY, df = dfs) +
    ns(EVENTS_PFY1, df = dfs) +
    ns(ATHLETICS_TICKET_YEARS, df = dfs) +
    ns(YEARS_SINCE_ATHLETICS_TICKETS, df = dfs) +
    ns(RECORD_YR, df = dfs) +
    ns(YEARS_SINCE_MAX_CASH_YR, df = dfs) +
    GIVING_MAX_CASH_MO +
    KSM_PROSPECT +
    ns(VISITORS_5FY, df = dfs) +
    LOYAL_5_PCT_CASH +
    UPGRADE3_CASH +
    VELOCITY3_LIN_CASH +
    SPOUSE_ALUM
  , data = traindat %>% mutate(
    YEARS_SINCE_FIRST_GIFT = 2016 - ifelse(GIVING_FIRST_YEAR > 0, GIVING_FIRST_YEAR, 2017)
    , YEARS_SINCE_ATHLETICS_TICKETS = 2016 - ifelse(ATHLETICS_TICKET_LAST > 0, ATHLETICS_TICKET_LAST, 2017)
    , YEARS_SINCE_MAX_CASH_YR = 2016 - ifelse(GIVING_MAX_CASH_YR > 0, GIVING_MAX_CASH_YR, 2017)
  )
  , family = 'binomial'
)
```
```{r}
summary(glm_st_splines)
```

```{r}
termplot(glm_st_splines)
```

Some more thoughts.

  * Don't put splines on the already-transformed giving variables
  * Consider the standard square root variance stabilizing transformation for counts
  * Try using "years since last (behavior) year"

```{r}
glm_st_splines <- glm(
  rv.gave ~
    PROGRAM_GROUP +
    PREF_ADDR_TYPE_CODE +
    HOUSEHOLD_CONTINENT +
    BUS_IS_EMPLOYED +
    HAS_HOME_ADDR +
    HAS_HOME_PHONE +
    # ns(YEARS_SINCE_FIRST_GIFT, 2) +
    # GIVING_FIRST_YEAR_CASH_AMT +
    # GIVING_MAX_PLEDGE_AMT +
    GIVING_CASH_TOTAL +
    # GIVING_PLEDGE_TOTAL +
    # GIVING_CRU_TOTAL +
    # sqrt(GIFTS_ALLOCS_SUPPORTED) +
    sqrt(GIFTS_FYS_SUPPORTED) +
    # sqrt(GIFTS_CASH) +
    sqrt(GIFTS_PLEDGES) +
    CASH_PFY1 +
    CASH_PFY2 +
    CASH_PFY3 +
    CASH_PFY4 +
    CASH_PFY5 +
    CRU_GIVING_SEGMENT +
    # EVALUATION_LOWER_BOUND +
    # UOR_LOWER_BOUND +
    # sqrt(MONTHS_ASSIGNED) +
    # sqrt(COMMITTEE_NU_DISTINCT) +
    # sqrt(COMMITTEE_NU_YEARS) +
    # sqrt(COMMITTEE_KSM_DISTINCT) +
    # sqrt(EVENTS_PREV_3_FY) +
    sqrt(EVENTS_CFY) +
    # sqrt(EVENTS_PFY1) +
    # sqrt(ATHLETICS_TICKET_YEARS) +
    # YEARS_SINCE_ATHLETICS_TICKETS +
    ns(RECORD_YR, df = 5) +
    YEARS_SINCE_MAX_CASH_YR +
    GIVING_MAX_CASH_MO +
    # KSM_PROSPECT +
    # sqrt(VISITORS_5FY) +
    LOYAL_5_PCT_CASH +
    # UPGRADE3_CASH +
    # VELOCITY3_LIN_CASH +
    SPOUSE_ALUM
  , data = traindat %>% mutate(
    YEARS_SINCE_FIRST_GIFT = 2016 - ifelse(GIVING_FIRST_YEAR > 0, GIVING_FIRST_YEAR, 2017)
    , YEARS_SINCE_MAX_CASH_YR = 2016 - ifelse(GIVING_MAX_CASH_YR > 0, GIVING_MAX_CASH_YR, 2017)
  )
  , family = 'binomial'
)
```
```{r}
summary(glm_st_splines)
```

Again, similar AIC.

### Penalized logistic regression, dropped variables

Fit a logistic regression model with the ridge penalizer using the same subset of variables chosen in the previous step.

```{r}
glm_ridge_cv <- cv.glmnet(
  rv.gave ~
    PROGRAM_GROUP +
    PREF_ADDR_TYPE_CODE +
    HOUSEHOLD_CONTINENT +
    BUS_IS_EMPLOYED +
    HAS_HOME_ADDR +
    HAS_HOME_PHONE +
    # ns(YEARS_SINCE_FIRST_GIFT, 2) +
    # GIVING_FIRST_YEAR_CASH_AMT +
    # GIVING_MAX_PLEDGE_AMT +
    GIVING_CASH_TOTAL +
    # GIVING_PLEDGE_TOTAL +
    # GIVING_CRU_TOTAL +
    # sqrt(GIFTS_ALLOCS_SUPPORTED) +
    sqrt(GIFTS_FYS_SUPPORTED) +
    # sqrt(GIFTS_CASH) +
    sqrt(GIFTS_PLEDGES) +
    CASH_PFY1 +
    CASH_PFY2 +
    CASH_PFY3 +
    CASH_PFY4 +
    CASH_PFY5 +
    CRU_GIVING_SEGMENT +
    # EVALUATION_LOWER_BOUND +
    # UOR_LOWER_BOUND +
    # sqrt(MONTHS_ASSIGNED) +
    # sqrt(COMMITTEE_NU_DISTINCT) +
    # sqrt(COMMITTEE_NU_YEARS) +
    # sqrt(COMMITTEE_KSM_DISTINCT) +
    # sqrt(EVENTS_PREV_3_FY) +
    sqrt(EVENTS_CFY) +
    # sqrt(EVENTS_PFY1) +
    # sqrt(ATHLETICS_TICKET_YEARS) +
    # YEARS_SINCE_ATHLETICS_TICKETS +
    ns(RECORD_YR, df = 5) +
    YEARS_SINCE_MAX_CASH_YR +
    GIVING_MAX_CASH_MO +
    # KSM_PROSPECT +
    # sqrt(VISITORS_5FY) +
    LOYAL_5_PCT_CASH +
    # UPGRADE3_CASH +
    # VELOCITY3_LIN_CASH +
    SPOUSE_ALUM
  , data = traindat %>% mutate(
    YEARS_SINCE_FIRST_GIFT = 2016 - ifelse(GIVING_FIRST_YEAR > 0, GIVING_FIRST_YEAR, 2017)
    , YEARS_SINCE_ATHLETICS_TICKETS = 2016 - ifelse(ATHLETICS_TICKET_LAST > 0, ATHLETICS_TICKET_LAST, 2017)
    , YEARS_SINCE_MAX_CASH_YR = 2016 - ifelse(GIVING_MAX_CASH_YR > 0, GIVING_MAX_CASH_YR, 2017)
  )
  , family = 'binomial'
  , alpha = 0 # Ridge penalty
)
```

Compare coefficients between the penalized and unpenalized models.

```{r, warning = FALSE, fig.width = 12, fig.height = 6}
full_join(
    data.frame(var = coef(glm_st_splines) %>% names(), unpenalized = coef(glm_st_splines))
  , data.frame(var = coef(glm_ridge_cv)[, 1] %>% names(), shrinkage = coef(glm_ridge_cv)[, 1])
  , by = c('var', 'var')
) %>% gather(model, 'coefficient', 2:3) %>%
  na.omit() %>%
  arrange(abs(coefficient) %>% desc()) %>%
  ggplot(aes(x = var %>% reorder(-abs(coefficient)), y = coefficient, color = model)) +
  geom_hline(yintercept = 0, color = 'darkgray') +
  geom_point(alpha = .5) +
  scale_y_continuous(trans = 'neg_sqrt', breaks = c(-50, -40, -30, seq(-20, 20, by = 5), -2, -.5, .5, 2)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .3)
            , panel.grid.minor = element_line(linetype = 'dotted')) +
  labs(x = 'var')
```

The ridge penalty leads to fairly aggressive coefficient shrinkage.

### Comparison

```{r}
# Holdout data with new variables
holdout_new <- holdoutdat %>% mutate(
    YEARS_SINCE_FIRST_GIFT = 2016 - ifelse(GIVING_FIRST_YEAR > 0, GIVING_FIRST_YEAR, 2017)
    , YEARS_SINCE_ATHLETICS_TICKETS = 2016 - ifelse(ATHLETICS_TICKET_LAST > 0, ATHLETICS_TICKET_LAST, 2017)
    , YEARS_SINCE_MAX_CASH_YR = 2016 - ifelse(GIVING_MAX_CASH_YR > 0, GIVING_MAX_CASH_YR, 2017)
  )
# Threshold
theta1 <- sum(traindat$rv.gave) / nrow(traindat)
# Calculations
tmp.ns <- conf_matrix(glm_standard, newdata = holdoutdat, threshold = theta1)
tmp.s <- conf_matrix(glm_st_splines, newdata = holdout_new, threshold = theta1)
tmp.rs <- conf_matrix_glmnet(glm_ridge_cv, newdata = holdout_new, rv = 'rv.gave', threshold = theta1)
# Data frame
model_compare <- cbind(
  glm_baseline_err
  , glm_nospline = c(tmp.ns$err, tmp.ns$prec, tmp.ns$sens, tmp.ns$F1)
  , glm_spline = c(tmp.s$err, tmp.s$prec, tmp.s$sens, tmp.ns$F1)
  , glm_ridge = c(tmp.rs$err, tmp.rs$prec, tmp.rs$sens, tmp.rs$F1)
)
remove(tmp.ns, tmp.s, tmp.rs)
```
```{r}
print(model_compare)
```

With threshold $\theta =$ `r theta1 %>% round(3) %>% I()` the glm_ridge model is the winner.

```{r}
# Calculations
tmp.ns <- conf_matrix(glm_standard, newdata = holdoutdat)
tmp.s <- conf_matrix(glm_st_splines, newdata = holdout_new)
tmp.rs <- conf_matrix_glmnet(glm_ridge_cv, newdata = holdout_new, rv = 'rv.gave')
# Data frame
model_compare <- cbind(
  glm_baseline_err
  , glm_nospline = c(tmp.ns$err, tmp.ns$prec, tmp.ns$sens, tmp.ns$F1)
  , glm_spline = c(tmp.s$err, tmp.s$prec, tmp.s$sens, tmp.ns$F1)
  , glm_ridge = c(tmp.rs$err, tmp.rs$prec, tmp.rs$sens, tmp.rs$F1)
)
remove(tmp.ns, tmp.s, tmp.rs)
```
```{r}
print(model_compare)
```

But with a decision threshold of $\theta =$ 0.5 the standard glm performs somewhat better, minimizing false negatives.

Consider the calibration plots.

```{r, fig.width = 10, fig.height = 6}
smooth.method <- 'loess'
glm_preds <- data.frame(
  class = (holdoutdat[, 1] + 0) %>% unlist()
  , ridge.baseline = predict(glm_ridge_baseline_model, newdata = holdout_new, type = 'response')
  , nospline = predict(glm_standard, newdata = holdout_new, type = 'response')
  , spline = predict(glm_st_splines, newdata = holdout_new, type = 'response')
  , ridge = predict(glm_ridge_cv, newdata = holdout_new, type = 'response')
) %>% setNames(
  c('class', 'ridge.baseline', 'nospline', 'spline', 'ridge')
) %>% gather(
  'model', 'prediction', ridge.baseline:ridge
)
# Plotting
glm_preds %>%
  ggplot(aes(x = prediction, y = class, group = model, color = model)) +
  geom_point(color = 'black', alpha  = .1) +
  geom_smooth(method = smooth.method, alpha = .5) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = paste0('Predictions with OOS smoother (', smooth.method, ')'), color = 'model'
       , x = 'predicted probability'
       , y = 'observed probability')
```

Interestingly, out-of-box baseline ridge regression outperforms the ridge regression model with fewer explanatory variables. Between these four I'd take the spline glm due to its interpretability.

We can also look at the ROC curves.

```{r, fig.width = 8, fig.height = 8}
rocdat <- cbind(model = 'ridge.baseline', roc_matrix_gen(glm_ridge_baseline_model, data = holdout_new)) %>%
  rbind(cbind(model = 'nospline', roc_matrix_gen(glm_standard, data = holdout_new))) %>%
  rbind(cbind(model = 'spline', roc_matrix_gen(glm_st_splines, data = holdout_new))) %>%
  rbind(cbind(model = 'ridge', roc_matrix_gen(glm_ridge_cv, data = holdout_new)))
# Plot results
rocdat %>%
  ggplot(aes(x = FPR, y = TPR, color = model)) +
  geom_line(size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed', col = 'black') +
  scale_x_continuous(breaks = seq(0, 1, by = .1), expand = c(0, 0)) +
  scale_y_continuous(breaks = seq(0, 1, by = .1), expand = c(0, 0)) +
  coord_equal() +
  labs(title = 'ROC plot')
```

Computing the AUC:

```{r}
data.frame(
    ridge.baseline = with(
        rocdat %>% filter(model == 'ridge.baseline')
      , sum(1/nrow(holdoutdat) * TPR)
    )
  , nospline = with(
        rocdat %>% filter(model == 'nospline')
      , sum(1/nrow(holdoutdat) * TPR)
    )
  , spline = with(
        rocdat %>% filter(model == 'spline')
      , sum(1/nrow(holdoutdat) * TPR)
    )
  , ridge = with(
        rocdat %>% filter(model == 'ridge')
      , sum(1/nrow(holdoutdat) * TPR)
    )
)
```

These are almost indistinguishable, but again the spline glm appears to be a reasonable choice.

# Regression model

I [found previously](https://cdn.rawgit.com/phively/ksm-models/c58e8065/pg-cultivation-score-fy18/02%20Cultivation%20score%20weights.nb.html) that linear regression actually works pretty well for predicting cumulative giving amounts, particularly when conditioning on donor status (thereby excluding all 0 entries).

$$ E \left(\text{log giving | donor, covariates} \right) = E\left(\text{log}\left(Y_i\right)\right) = X_i \boldsymbol{\beta} $$

Here, $Y_i = \text{FY18Giving}_i + \text{FY17Giving}_i$ and the training data $X_i$ includes the observations where $Y_i > 0$.

## Variable selection

I'll again pre-screen variables with the Boruta algorithm, but this time the response variable is continuous and only donors will be included.

```{r, cache = TRUE}
# Sample rows
prop = 1 # Proportion of data to sample; 1 for all donors
set.seed(7968177)
# Include only entities who gave
samp <- sample_n(
  modeling.data %>% filter(rv.gave)
  , size = nrow(modeling.data %>% filter(rv.gave)) * prop
  , replace = FALSE
) %>% select(
  -rv.gave, -ID_NUMBER, -HOUSEHOLD_ID, -INSTITUTIONAL_SUFFIX, -DEGREES_CONCAT
)

# Run Boruta algorithm, but only on entities that gave
rf.vars.lm <- Boruta(
    y = log10(samp$rv.amt)
    , x = samp %>% select(-rv.amt)
    , seed = 3371662
  )
```
```{r}
rf.vars.lm %>% print()
```

Save the results.

```{r}
save(rf.vars.lm, file = 'data/rf.vars.lm.Rdata')
```

```{r, fig.width = 8, fig.height = 20}
(lmod_plot <- rf.vars.lm %>% Borutadata() %>% Borutaplotter())
```

Some unsurprising findings:

  * Again, past giving is the best predictor of future giving, particularly past totals and upgrades (velocity)
  * Generally speaking, more recent giving is more predictive than less recent giving
  * Prospect assignment and length of assignment is predictive of future giving
  * UOR and evaluation rating are predictive
  * Gift club membership is predictive
  * Event attendance is predictive
  
And some surprising ones:

  * Max cash giving is more predictive than new gifts and commitments. Perhaps there's an unfulfilled pledge effect hidden away?
  * Giving 5 years ago is also pretty predictive - perhaps a paid off pledge effect?
  * Besides overall tenure, committee participation is not predictive
  * Donor Advised Fund giving amounts and stock gifts are not predictive
  * Contact information is not predictive

```{r}
(recommended.vars.lm <- TentativeRoughFix(rf.vars.lm))
```

```{r, fig.width = 16, fig.height = 16}
# Check variable correlations
recommended_vars_lm <- recommended.vars.lm$finalDecision[
  which(recommended.vars.lm$finalDecision == 'Confirmed')] %>% names()
numeric_vars_lm <- modeling.data %>%
  filter(rv.gave) %>%
  select(recommended_vars_lm) %>%
  select_if(is.numeric)
numeric_vars_lm %>% plot_corrs(textsize = 2)
```

  * Probably only one of AF or CRU by year should be kept.
  * Probably only one of visitors and visits should be kept.

## Cross-validation

The modeling data file is a slightly trimmed version of modeling.data.

```{r}
# Data file with variables removed
lmdat <- modeling.data %>%
  filter(rv.gave) %>%
  select(rv.amt, recommended_vars_lm) %>%
  select(
    -GIVING_AF_TOTAL
    , -KSM_GOS
    , -contains('AF_PFY')
    , -VELOCITY3_LIN_CASH
    , -VELOCITY_BINS_NGC
    , -VELOCITY_BINS_CASH
    , -VELOCITY3_CASH
    , -VELOCITY3_NGC
    , -GIFTS_CASH
    , -VISITORS_5FY
    , -FIRST_KSM_YEAR
    , -GIVING_PLEDGE_TOTAL
    , -GIVING_MAX_CASH_FY
    , -GIFTS_MATCHES
    , -CRU_STATUS
    , -BIRTH_DT
    , -SPOUSE_SUFFIX
    , -PAST_KSM_GOS_FLAG
    , -GIVING_MAX_PLEDGE_MO
    , -UPGRADE3_NGC
  ) %>% mutate(
    # Create spouse flag
    SPOUSE_ALUM = ifelse(SPOUSE_FIRST_KSM_YEAR > 0, 'TRUE', 'FALSE') %>% factor()
    # Lump together little-used continents
    , HOUSEHOLD_CONTINENT = fct_lump(HOUSEHOLD_CONTINENT, prop = .0075)
    # Lump together little-used regions
    , HOUSEHOLD_REGION = fct_collapse(
        HOUSEHOLD_REGION
        , 'NB' = 'INTL'
    )
  ) %>% mutate_if(
    # Numeric variables over 1E4 get a log10 transformation
    function(x) {
      ifelse(is.numeric(x), max(x) >= 1E4, FALSE)
    }
    , log10plus1
  ) %>% select(
    -SPOUSE_FIRST_KSM_YEAR
  )

# Cross-validation settings
folds = 10
reps = 5

# Withhold 10% of data as test set
xv <- KFoldXVal(lmdat, k = 2, prop = .1, seed = 1626452)
holdoutdat <- lmdat[xv[[1]], ]
traindat <- lmdat[xv[[2]], ]
remove(xv)
```

### Baseline linear regression

The model to beat will be a generic regression model with all variables previously identified as important.

```{r}
# Store timings
timestamps <- list()
# Store linear models
lm_baseline <- list()
# Seed for reproducibility
set.seed(2285286)

# Outer loop (repetitions)
for (rep in 1:reps) {
  # Status report 
  timestamp <- paste('+ Iteration', rep, 'beginning at:', Sys.time())
  print(timestamp)
  timestamps <- c(timestamps, timestamp)
  # Create cross-validation indices
  xv <- KFoldXVal(traindat, k = folds)
  # Inner loop (parallel cross-validation)
  models_out <- foreach(
    fold = 1:length(xv)
    , .combine = list
    , .multicombine = TRUE
    , .packages = c('dplyr', 'splines')
  ) %dopar% {
    # Fit temp model
      tmpmodel <- lm(
        rv.amt ~ .
        # Train while withholding some data
        , data = traindat[-xv[[fold]], ]
      )
      preds <- data.frame(
        prediction = predict(tmpmodel, newdata = traindat[xv[[fold]], ], type = 'response')
        , actual = traindat$rv.amt[xv[[fold]]]
      )
    # Return results
    return(
      list(
        rep = rep
        , fold = fold
        , model = tmpmodel
        , predictions = preds
      )
    )
  }
  # Write results to errors data frame
  lm_baseline <- c(lm_baseline, models_out)
  # Status report
  timestamp <- paste(' -Iteration', rep, 'ending at:   ', Sys.time())
  print(timestamp)
  timestamps <- c(timestamps, timestamp)
}
```

```{r, warning = FALSE, error = FALSE}
lm_models <- ListExtract(lm_baseline, 'model')
lm_preds <- ListExtract(lm_baseline, 'predictions')
n <- folds * reps
```
```{r, warning = FALSE, error = FALSE}
# r^2 plots
plot_r2(lm_models) +
  geom_text(y = seq(10, 150, length.out = n), label = 1:n, color = 'blue') 
```

The $r^2$ results are good for this application (anything over .5 or so is reasonable).

```{r, warning = FALSE, error = FALSE}
plot_mses(lm_models, lm_preds)
```

This is a pretty good result, and the out-sample MSE is only slightly worse than the in-sample MSE.

```{r, message = FALSE, warning = FALSE}
plot_resids(lm_models, lm_preds)$insample
```

```{r, warning = FALSE, error = FALSE}
plot_resids(lm_models, lm_preds)$outsample
```

On average the model slightly overestimates on the low end and underestimates on the high end.

```{r}
plot_qq(lm_models, lm_preds)$insample
```

```{r}
plot_qq(lm_models, lm_preds)$outsample
```

The Q-Q plots support the earlier observation that there's extra density in the tails.

Consider the model coefficients.

```{r, warning = FALSE, error = FALSE, fig.width = 12, fig.height = 6}
p.sig <- .05
plot_coefs(lm_models) +
  guides(color = FALSE) +
  scale_y_continuous(trans = 'neg_sqrt')
```

```{r, warning = FALSE, error = FALSE, rows.print = 1000}
coef_pm_table(lm_models, p.sig)
```

The table indicates the number of models in which the coefficient was significantly positive or negative (or not signficantly different from 0) at the $p =$ `r p.sig %>% I()` level.

 

### Linear regression with splines



### Ridge regression



### LASSO



### Comparison



# Conclusions
