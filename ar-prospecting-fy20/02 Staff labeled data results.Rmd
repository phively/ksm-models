---
title: "02 Staff labeled data results"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Goal

Quick analysis of the datasets labeled by staff, ultimately to create some sort of weighted training data set.

# Setup

```{r setup}
library(tidyverse)
library(wranglR)
library(readxl)
library(splines)
library(foreach)
```

```{r}
# Load master data
load(file = 'data/output/entities_modeled.Rdata')
```

```{r}
# Function to load and parse completed lists
parse_lists <- function(path) {
  files <- list.files(path)
  foreach(f = files, .combine = rbind, .multicombine = TRUE) %do% {
    # Load the data and extract contributor name
    data <- read_xlsx(path = paste0(path, '/', f), guess_max = 1E6)
    name <- str_match(f, '(?<=list for ).*(?=.xlsx)')
    data$name <- name
    # Keep only needed fields
    data %>%
      rename(Student = `Student Supporter?`, Connector = `Connector?`, Notable = `Notable?`, Interesting = `Interesting career/story`) %>%
      select(RANDOM_ID, name, Student, Connector, Notable, Interesting, Notes) %>%
      return()
  }
}

lists <- parse_lists(path = 'data/output/completed lists/')
```

# Initial exploration

```{r}
lists <- lists %>%
  mutate(
    across(.cols = name:Interesting, .fns = as.factor)
  )

lists %>% summary(maxsum = 1E3)
```

I can weight yes as 1, maybe as 0.5. Or just everything as 1?

```{r, rows.print = 100}
level_checker <- function(field) {
  lists %>%
    filter(!is.na({{ field }})) %>%
    group_by(name, {{ field}}) %>%
    summarise(n(), .groups = 'drop')
}
```

```{r, rows.print = 100}
level_checker(Student)
level_checker(Notable)
level_checker(Connector)
level_checker(Interesting)
```

Since all the possibles were from one person I'm inclined to treat them the same as yes.

```{r}
lists <- lists %>%
  mutate(
    across(.cols = Student:Interesting, .fns = function(x) {!is.na(x)})
  )
```

```{r}
lists %>% summary(maxsum = 1E3)
```

Finally, combine individual results back into the master data frame.

# Calibration names

A few people appeared on every list:

```{r}
calibration_raw <- entities_modeled %>%
  left_join(
    lists
    , by = c('RANDOM_ID' = 'RANDOM_ID')
  )

calibration <- calibration_raw %>%
  group_by(ID_NUMBER, RANDOM_ID, REPORT_NAME, INSTITUTIONAL_SUFFIX, quantile) %>%
  summarise(
    n = n()
    , student_supporter = sum(Student)
    , connector = sum(Connector)
    , notable = sum(Notable)
    , career = sum(Interesting)
    , .groups = 'drop'
  ) %>%
  filter(n > 1) %>%
  mutate(
    total = student_supporter + connector + notable + career
    , tag = case_when(
        quantile < .3 ~ '**'
        , quantile %>% between(.3, .7) ~ '+'
        , quantile > .7 ~ '--'
      )
  )
```

```{r, rows.print = 100}
calibration %>%
  select(-ID_NUMBER, -REPORT_NAME, -INSTITUTIONAL_SUFFIX, -n) %>%
  select(RANDOM_ID, quantile, tag, everything()) %>%
  arrange(desc(total))
```

Now that's interesting. People who are very interesting based on my initial (suspect) modeling process are consistently identified as such, while people in the middle or near the bottom are not easily identified.

Look at some of the data more in depth.

```{r}
entities_modeled %>%
  filter(RANDOM_ID %in% calibration$RANDOM_ID) %>%
  left_join(
    calibration %>% select(-ID_NUMBER, -REPORT_NAME, -INSTITUTIONAL_SUFFIX, -quantile, -n)
    , by = c('RANDOM_ID' = 'RANDOM_ID')
  ) %>%
  select(RANDOM_ID, quantile, student_supporter, connector, notable, career, total, tag, everything()) %>%
  arrange(desc(total)) %>%
  View()
```

Okay, it appears that the team is looking for indicators of engagement. If someone is completely unengaged, they are not being identified as someone who fits into one of the segments. I see both pros and cons to this approach.

# Inter-rater agreement

Can define rater agreement either as the RMSE between their own rating and the group average, or the dot product between their ratings.

Start with MSE. How often do raters agree with the group consensus? Compute the consensus for each category, then as usual find:

$$ RMSE = \sqrt{ \frac{\sum_{i=1}^{N} \left( x_i - \bar{x} \right)^2}{N} } $$


```{r, rows.print = 20}
# Count of distinct raters
n_raters <- calibration_raw$name %>% levels() %>% length()

f_rmse <- function(x, xbar) {
  sqrt({x - xbar}^2) %>%
    sum() %>%
    return()
}

calibration_comparison <- calibration_raw %>%
  filter(
    ID_NUMBER %in% calibration$ID_NUMBER
  ) %>%
  select(
    ID_NUMBER, RANDOM_ID, REPORT_NAME, INSTITUTIONAL_SUFFIX
    , quantile, name, Student, Connector, Notable, Interesting
  ) %>%
  group_by(ID_NUMBER) %>%
  mutate(
    avg_Student = sum(Student) / n_raters
    , avg_Connector = sum(Connector) / n_raters
    , avg_Notable = sum(Notable) / n_raters
    , avg_Interesting = sum(Interesting) / n_raters
  ) %>%
  # Compute MSE
  mutate(
    SE_Student = (Student - avg_Student)^2
    , SE_Connector = (Connector - avg_Connector)^2
    , SE_Notable = (Notable - avg_Notable)^2
    , SE_Interesting = (Interesting - avg_Interesting)^2
  ) %>%
  # Group by rater
  group_by(name) %>%
  summarise(
    Student = sqrt(sum(SE_Student) / n_raters)
    , Connector = sqrt(sum(SE_Connector) / n_raters)
    , Notable = sqrt(sum(SE_Notable) / n_raters)
    , Interesting = sqrt(sum(SE_Interesting) / n_raters)
    , .groups = 'drop'
  ) %>% mutate(
    summed = Student + Connector + Notable + Interesting
  ) %>%
  arrange(
    desc(summed)
  )

calibration_comparison %>% print()
```

```{r}
calibration_comparison %>%
  pivot_longer(
    cols = Student:Interesting
    , names_to = 'category'
  ) %>%
  ggplot(aes(x = category, y = value, color = name)) +
  geom_point() +
  geom_line(aes(group = name))
```

```{r}
calibration_comparison %>%
  pivot_longer(
    cols = Student:Interesting
    , names_to = 'Category'
  ) %>%
  ggplot(aes(x = Category, y = value)) +
  geom_boxplot(alpha = .5)
```

Kind of hard to interpret, but there is only really significant agreement for Notable. The others look a bit all over the place. Note that the medians are below .5 though, so it's not totally random.

As a first pass I could use 1 - RMSE as a weight for each person's ratings.

# Inter-rater correlations

First create a matrix encoding ratings for each combination of alum/staff.

# To Do

- [ ] Correlation matrix showing agreement between raters
- [ ] Scoring based on % of time they agree with the majority vote?
- [ ] Weighted overall lists based on scoring
- [ ] Response variable as a 4-element vector? One per student supporter, connector, notable, career? Or just 0-4 ranking of estimated sum? (This assumes each type of alum is equally interesting)